{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Turkey', 'is', 'my', 'country', 'i', 'love', 'it']\n"
     ]
    }
   ],
   "source": [
    "text = 'Turkey is my country i love it'\n",
    "\n",
    "text_tokens = word_tokenize(text)\n",
    "print(text_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Turkey', 'country', 'love']\n"
     ]
    }
   ],
   "source": [
    "tokens_without_stopsword = [word for word in text_tokens if word not in stopwords.words()]\n",
    "print(tokens_without_stopsword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turkey\n",
      "country\n",
      "love\n"
     ]
    }
   ],
   "source": [
    "for word in text_tokens:\n",
    "    if word not in stopwords.words():\n",
    "        print(word)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.rcParams[\"figure.figsize\"] = (10,6)\n",
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/veyselaytekin/Desktop/Data_Science/Deep_Learning_folder/Deep_Learning/Techpro/NLP/airline_tweets.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/_9/ph76r20j4ss1dpbdh33_j0fm0000gn/T/ipykernel_2490/1368270783.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/veyselaytekin/Desktop/Data_Science/Deep_Learning_folder/Deep_Learning/Techpro/NLP/airline_tweets.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 932\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1214\u001b[0m             \u001b[0;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[1;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    784\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/veyselaytekin/Desktop/Data_Science/Deep_Learning_folder/Deep_Learning/Techpro/NLP/airline_tweets.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/veyselaytekin/Desktop/Data_Science/Deep_Learning_folder/Deep_Learning/Techpro/NLP/airline_tweets.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text= \"Awesome!!!, This is fantastic!!!. We are very pleased...3456\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/veyselaytekin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['awesome!!', '!, this is fantastic!!!.', 'we are very pleased...3456']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_token = sent_tokenize(sample_text.lower())\n",
    "sentence_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sentence_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['awesome',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " ',',\n",
       " 'this',\n",
       " 'is',\n",
       " 'fantastic',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '.',\n",
       " 'we',\n",
       " 'are',\n",
       " 'very',\n",
       " 'pleased',\n",
       " '...',\n",
       " '3456']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_token = word_tokenize(sample_text.lower())\n",
    "word_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['awesome', 'this', 'is', 'fantastic', 'we', 'are', 'very', 'pleased']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_without_punct = [w for w in word_token if w.isalpha()]\n",
    "tokens_without_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['awesome', 'this', 'is', 'fantastic', 'we', 'are', 'very', 'pleased', '3456']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_without_punct = [w for w in word_token if w.isalnum()]\n",
    "tokens_without_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "232"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopword = stopwords.words('german')\n",
    "len(stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopword = stopwords.words('english')\n",
    "len(stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text= \"Awesome!!!, This is not  fantastic!!!. We are very pleased...3456\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Awesome',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " ',',\n",
       " 'This',\n",
       " 'is',\n",
       " 'not',\n",
       " 'fantastic',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '.',\n",
       " 'We',\n",
       " 'are',\n",
       " 'very',\n",
       " 'pleased',\n",
       " '...',\n",
       " '3456']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text_list = word_tokenize(sample_text)\n",
    "sample_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sample_text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Awesome', 'This', 'is', 'not', 'fantastic', 'We', 'are', 'very', 'pleased']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_without_punct = [w for w in sample_text_list if w.isalpha()]\n",
    "tokens_without_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Awesome', 'This', 'fantastic', 'We', 'pleased']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_without_stopsword = [w for w in tokens_without_punct if w not in stopwords.words('english')]\n",
    "tokens_without_stopsword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/veyselaytekin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/veyselaytekin/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WordNetLemmatizer().lemmatize('running', pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'drive'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WordNetLemmatizer().lemmatize('drove', pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'running'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WordNetLemmatizer().lemmatize('running', pos='n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Awesome', 'This', 'is', 'not', 'fantastic', 'We', 'are', 'very', 'pleased']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_without_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Awesome', 'This', 'be', 'not', 'fantastic', 'We', 'be', 'very', 'please']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem = [WordNetLemmatizer().lemmatize(w,pos = 'v') for w in tokens_without_punct ]\n",
    "lem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'studi'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PorterStemmer().stem('studies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'study'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WordNetLemmatizer().lemmatize('studies')\n",
    "#it makes sense than stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['awesom', 'thi', 'is', 'not', 'fantast', 'we', 'are', 'veri', 'pleas']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem = [PorterStemmer().stem(t) for t in tokens_without_punct]\n",
    "stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Awesome-This-be-not-fantastic-We-be-very-please'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'-'.join(lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_text(data):\n",
    "    text_token = word_tokenize(data.lower())\n",
    "\n",
    "    tokens_without_punct = [w for w in text_token if w.isalpha()]\n",
    "\n",
    "    tokens_without_stopsword = [w for w in tokens_without_punct if w not in stopwords]\n",
    "\n",
    "    text_cleaned = [WordNetLemmatizer().lemmatize(w) for w in tokens_without_stopsword]\n",
    "\n",
    "    return ' '.join(text_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Awesome!!!, This is not  fantastic!!!. We are very pleased...3456'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'awesome fantastic pleased'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaning_text(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    awesome fantastic pleased\n",
       "dtype: object"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(sample_text).apply(cleaning_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ['no','not']:\n",
    "    stopwords.remove(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "list.remove(x): x not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/_9/ph76r20j4ss1dpbdh33_j0fm0000gn/T/ipykernel_2490/2050557860.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'no'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'not'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: list.remove(x): x not in list"
     ]
    }
   ],
   "source": [
    "# stopwords = stopwords.words('english')\n",
    "\n",
    "for i in ['no','not']:\n",
    "    stopwords.remove(i)\n",
    "\n",
    "\n",
    "def cleaning_text_for_sentiment_analyses(data):\n",
    "    text = data.replace(\"'\", '')\n",
    "\n",
    "    text_token = word_tokenize(text.lower())\n",
    "\n",
    "    tokens_without_punct = [w for w in text_token if w.isalpha()]\n",
    "\n",
    "    tokens_without_stopsword = [w for w in tokens_without_punct if w not in stopwords]\n",
    "\n",
    "    text_cleaned = [WordNetLemmatizer().lemmatize(t) for t in tokens_without_stopsword]\n",
    "\n",
    "\n",
    "\n",
    "    return ' '.join(text_cleaned)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_text_for_sentiment_analyses(data):\n",
    "    text = data.replace(\"'\", '')\n",
    "\n",
    "    text_token = word_tokenize(text.lower())\n",
    "\n",
    "    tokens_without_punct = [w for w in text_token if w.isalpha()]\n",
    "\n",
    "    tokens_without_stopsword = [w for w in tokens_without_punct if w not in stopwords]\n",
    "\n",
    "    text_cleaned = [WordNetLemmatizer().lemmatize(t) for t in tokens_without_stopsword]\n",
    "\n",
    "\n",
    "\n",
    "    return ' '.join(text_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Awesome!!!, This is not  fantastic!!!. We are very pleased...3456'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    awesome not fantastic pleased\n",
       "dtype: object"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(sample_text).apply(cleaning_text_for_sentiment_analyses)\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"airline_tweets.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14635</th>\n",
       "      <td>positive</td>\n",
       "      <td>@AmericanAir thank you we got on a different f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14636</th>\n",
       "      <td>negative</td>\n",
       "      <td>@AmericanAir leaving over 20 minutes Late Flig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14637</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@AmericanAir Please bring American Airlines to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14638</th>\n",
       "      <td>negative</td>\n",
       "      <td>@AmericanAir you have my money, you change my ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14639</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@AmericanAir we have 8 ppl so we need 2 know h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14640 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      airline_sentiment                                               text\n",
       "0               neutral                @VirginAmerica What @dhepburn said.\n",
       "1              positive  @VirginAmerica plus you've added commercials t...\n",
       "2               neutral  @VirginAmerica I didn't today... Must mean I n...\n",
       "3              negative  @VirginAmerica it's really aggressive to blast...\n",
       "4              negative  @VirginAmerica and it's a really big bad thing...\n",
       "...                 ...                                                ...\n",
       "14635          positive  @AmericanAir thank you we got on a different f...\n",
       "14636          negative  @AmericanAir leaving over 20 minutes Late Flig...\n",
       "14637           neutral  @AmericanAir Please bring American Airlines to...\n",
       "14638          negative  @AmericanAir you have my money, you change my ...\n",
       "14639           neutral  @AmericanAir we have 8 ppl so we need 2 know h...\n",
       "\n",
       "[14640 rows x 2 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[['airline_sentiment','text']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                              virginamerica dhepburn said\n",
       "1        virginamerica plus youve added commercial expe...\n",
       "2        virginamerica didnt today must mean need take ...\n",
       "3        virginamerica really aggressive blast obnoxiou...\n",
       "4                       virginamerica really big bad thing\n",
       "                               ...                        \n",
       "14635       americanair thank got different flight chicago\n",
       "14636    americanair leaving minute late flight no warn...\n",
       "14637            americanair please bring american airline\n",
       "14638    americanair money change flight dont answer ph...\n",
       "14639    americanair ppl need know many seat next fligh...\n",
       "Name: text, Length: 14640, dtype: object"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'] = df['text'].apply(cleaning_text_for_sentiment_analyses)\n",
    "df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                              virginamerica dhepburn said\n",
       "1        virginamerica plus youve added commercial expe...\n",
       "2        virginamerica didnt today must mean need take ...\n",
       "3        virginamerica really aggressive blast obnoxiou...\n",
       "4                       virginamerica really big bad thing\n",
       "                               ...                        \n",
       "14635       americanair thank got different flight chicago\n",
       "14636    americanair leaving minute late flight no warn...\n",
       "14637            americanair please bring american airline\n",
       "14638    americanair money change flight dont answer ph...\n",
       "14639    americanair ppl need know many seat next fligh...\n",
       "Name: text, Length: 14640, dtype: object"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "for i in df['text']:\n",
    "    if i == 'virginamerica dhepburn said':\n",
    "        print('True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['text']\n",
    "y = df['airline_sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.6, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X_train_count = vectorizer.fit_transform(X_train)\n",
    "X_test_count = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aa',\n",
       " 'aaadvantage',\n",
       " 'aaalwayslate',\n",
       " 'aadavantage',\n",
       " 'aadv',\n",
       " 'aadvantage',\n",
       " 'aafail',\n",
       " 'aal',\n",
       " 'aaron',\n",
       " 'aarp',\n",
       " 'abandon',\n",
       " 'abc',\n",
       " 'abcnetwork',\n",
       " 'abcnews',\n",
       " 'abi',\n",
       " 'abigailedge',\n",
       " 'ability',\n",
       " 'able',\n",
       " 'aboard',\n",
       " 'abq',\n",
       " 'absolute',\n",
       " 'absolutely',\n",
       " 'absoulutely',\n",
       " 'absurd',\n",
       " 'absurdly',\n",
       " 'abt',\n",
       " 'abuse',\n",
       " 'abused',\n",
       " 'abysmal',\n",
       " 'accelerate',\n",
       " 'accept',\n",
       " 'acceptable',\n",
       " 'accepting',\n",
       " 'access',\n",
       " 'accessible',\n",
       " 'accessing',\n",
       " 'accidentally',\n",
       " 'accommodate',\n",
       " 'accommodation',\n",
       " 'accompany',\n",
       " 'accomplish',\n",
       " 'accomplished',\n",
       " 'according',\n",
       " 'account',\n",
       " 'accountability',\n",
       " 'acct',\n",
       " 'accts',\n",
       " 'accurate',\n",
       " 'accurately',\n",
       " 'accuratetraveltimes',\n",
       " 'achieves',\n",
       " 'acknowledge',\n",
       " 'acknowledgement',\n",
       " 'acknowledgment',\n",
       " 'acpt',\n",
       " 'acquisition',\n",
       " 'across',\n",
       " 'act',\n",
       " 'action',\n",
       " 'activate',\n",
       " 'active',\n",
       " 'actual',\n",
       " 'actually',\n",
       " 'actualy',\n",
       " 'acu',\n",
       " 'acy',\n",
       " 'ad',\n",
       " 'adam',\n",
       " 'add',\n",
       " 'added',\n",
       " 'adding',\n",
       " 'addition',\n",
       " 'additional',\n",
       " 'additionally',\n",
       " 'additonal',\n",
       " 'address',\n",
       " 'addressed',\n",
       " 'adjacent',\n",
       " 'adjusting',\n",
       " 'adjustment',\n",
       " 'admiral',\n",
       " 'admiralsclub',\n",
       " 'admit',\n",
       " 'admitted',\n",
       " 'adopted',\n",
       " 'adopting',\n",
       " 'adore',\n",
       " 'adress',\n",
       " 'adult',\n",
       " 'advan',\n",
       " 'advance',\n",
       " 'advantage',\n",
       " 'advertise',\n",
       " 'advertising',\n",
       " 'advice',\n",
       " 'advis',\n",
       " 'advise',\n",
       " 'advising',\n",
       " 'advisory',\n",
       " 'advsry',\n",
       " 'aerojobmarket',\n",
       " 'aeroport',\n",
       " 'aex',\n",
       " 'affair',\n",
       " 'affected',\n",
       " 'affecting',\n",
       " 'afford',\n",
       " 'affordable',\n",
       " 'african',\n",
       " 'aft',\n",
       " 'afterall',\n",
       " 'afternoon',\n",
       " 'agcommunity',\n",
       " 'age',\n",
       " 'aged',\n",
       " 'agency',\n",
       " 'agent',\n",
       " 'aggravation',\n",
       " 'aggressive',\n",
       " 'aging',\n",
       " 'agnt',\n",
       " 'ago',\n",
       " 'agree',\n",
       " 'agreement',\n",
       " 'agts',\n",
       " 'ah',\n",
       " 'aha',\n",
       " 'ahead',\n",
       " 'ahem',\n",
       " 'ahhhh',\n",
       " 'ahhhhh',\n",
       " 'ahold',\n",
       " 'aim',\n",
       " 'aint',\n",
       " 'air',\n",
       " 'airborne',\n",
       " 'airbus',\n",
       " 'airbusintheus',\n",
       " 'aircanada',\n",
       " 'aircargo',\n",
       " 'aircraft',\n",
       " 'aired',\n",
       " 'airfare',\n",
       " 'airline',\n",
       " 'airlinegeeks',\n",
       " 'airlineguys',\n",
       " 'airliner',\n",
       " 'airlinesecurity',\n",
       " 'airnzusa',\n",
       " 'airplane',\n",
       " 'airplanemodewason',\n",
       " 'airport',\n",
       " 'airside',\n",
       " 'airspace',\n",
       " 'airway',\n",
       " 'aisle',\n",
       " 'aka',\n",
       " 'al',\n",
       " 'alamo',\n",
       " 'alarm',\n",
       " 'alaska',\n",
       " 'albany',\n",
       " 'albanyairport',\n",
       " 'albeit',\n",
       " 'albertbreer',\n",
       " 'album',\n",
       " 'albuquer',\n",
       " 'albuquerque',\n",
       " 'alcohol',\n",
       " 'alert',\n",
       " 'alerted',\n",
       " 'alex',\n",
       " 'ali',\n",
       " 'aligned',\n",
       " 'alittlebetter',\n",
       " 'allan',\n",
       " 'allende',\n",
       " 'allergic',\n",
       " 'allergy',\n",
       " 'allgood',\n",
       " 'allow',\n",
       " 'allowance',\n",
       " 'allowed',\n",
       " 'allowing',\n",
       " 'allows',\n",
       " 'allrepresentativesbusy',\n",
       " 'alls',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'alot',\n",
       " 'already',\n",
       " 'alright',\n",
       " 'also',\n",
       " 'alstdi',\n",
       " 'alt',\n",
       " 'alternate',\n",
       " 'alternative',\n",
       " 'alternatively',\n",
       " 'although',\n",
       " 'altitude',\n",
       " 'alway',\n",
       " 'always',\n",
       " 'alwaysdelayed',\n",
       " 'alwayslate',\n",
       " 'amateur',\n",
       " 'amazed',\n",
       " 'amazing',\n",
       " 'amazings',\n",
       " 'amazon',\n",
       " 'ambivalence',\n",
       " 'america',\n",
       " 'american',\n",
       " 'americanair',\n",
       " 'americanairlines',\n",
       " 'americanairlinesfail',\n",
       " 'americanairsucks',\n",
       " 'americanforlife',\n",
       " 'americanisbetter',\n",
       " 'americant',\n",
       " 'americanview',\n",
       " 'amex',\n",
       " 'amexserve',\n",
       " 'amm',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'amount',\n",
       " 'amp',\n",
       " 'amsterdam',\n",
       " 'amtrak',\n",
       " 'ana',\n",
       " 'analytics',\n",
       " 'anamarketers',\n",
       " 'anaphylaxis',\n",
       " 'andrew',\n",
       " 'andrewfallis',\n",
       " 'android',\n",
       " 'angel',\n",
       " 'angeles',\n",
       " 'angelo',\n",
       " 'angry',\n",
       " 'angrybird',\n",
       " 'ann',\n",
       " 'anna',\n",
       " 'annebevi',\n",
       " 'anni',\n",
       " 'anniversary',\n",
       " 'annnndddd',\n",
       " 'annnnddddd',\n",
       " 'annnnnd',\n",
       " 'announce',\n",
       " 'announced',\n",
       " 'announcement',\n",
       " 'announcer',\n",
       " 'announces',\n",
       " 'announcing',\n",
       " 'annoyed',\n",
       " 'annoying',\n",
       " 'annricord',\n",
       " 'annual',\n",
       " 'another',\n",
       " 'anotherdisappointment',\n",
       " 'answer',\n",
       " 'answered',\n",
       " 'answering',\n",
       " 'answerphone',\n",
       " 'anthony',\n",
       " 'anticipate',\n",
       " 'antigua',\n",
       " 'antonio',\n",
       " 'anxiety',\n",
       " 'anxious',\n",
       " 'anybody',\n",
       " 'anyhelp',\n",
       " 'anymore',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anytime',\n",
       " 'anyway',\n",
       " 'anyways',\n",
       " 'anywhere',\n",
       " 'apart',\n",
       " 'apathetic',\n",
       " 'apeared',\n",
       " 'api',\n",
       " 'apologise',\n",
       " 'apologize',\n",
       " 'apologized',\n",
       " 'apologizes',\n",
       " 'apologizing',\n",
       " 'apology',\n",
       " 'app',\n",
       " 'appalled',\n",
       " 'apparent',\n",
       " 'apparently',\n",
       " 'appear',\n",
       " 'appeared',\n",
       " 'appears',\n",
       " 'appease',\n",
       " 'applaud',\n",
       " 'apple',\n",
       " 'applepay',\n",
       " 'appleton',\n",
       " 'applicable',\n",
       " 'application',\n",
       " 'applied',\n",
       " 'apply',\n",
       " 'applying',\n",
       " 'appointed',\n",
       " 'appointment',\n",
       " 'appreciate',\n",
       " 'appreciated',\n",
       " 'appreciates',\n",
       " 'appreciation',\n",
       " 'approaching',\n",
       " 'appropriate',\n",
       " 'appropriately',\n",
       " 'appropriation',\n",
       " 'approval',\n",
       " 'approve',\n",
       " 'approved',\n",
       " 'approx',\n",
       " 'approximate',\n",
       " 'approximately',\n",
       " 'apps',\n",
       " 'april',\n",
       " 'apt',\n",
       " 'aquadilla',\n",
       " 'ar',\n",
       " 'ardent',\n",
       " 'area',\n",
       " 'arent',\n",
       " 'argentina',\n",
       " 'argue',\n",
       " 'argued',\n",
       " 'argument',\n",
       " 'arizona',\n",
       " 'arm',\n",
       " 'armrest',\n",
       " 'arose',\n",
       " 'around',\n",
       " 'arrange',\n",
       " 'arranged',\n",
       " 'arrangement',\n",
       " 'arrival',\n",
       " 'arrive',\n",
       " 'arrived',\n",
       " 'arrives',\n",
       " 'arriving',\n",
       " 'arrogant',\n",
       " 'article',\n",
       " 'aruba',\n",
       " 'arvls',\n",
       " 'asap',\n",
       " 'ase',\n",
       " 'asgmnt',\n",
       " 'ashamed',\n",
       " 'ashley',\n",
       " 'asia',\n",
       " 'ask',\n",
       " 'askamex',\n",
       " 'asked',\n",
       " 'asking',\n",
       " 'askpaypal',\n",
       " 'asks',\n",
       " 'asleep',\n",
       " 'aspen',\n",
       " 'asshole',\n",
       " 'assign',\n",
       " 'assigned',\n",
       " 'assignment',\n",
       " 'assist',\n",
       " 'assistance',\n",
       " 'assistant',\n",
       " 'assisting',\n",
       " 'associated',\n",
       " 'assuage',\n",
       " 'assume',\n",
       " 'assurance',\n",
       " 'assure',\n",
       " 'assured',\n",
       " 'assuring',\n",
       " 'astoria',\n",
       " 'astounded',\n",
       " 'astounds',\n",
       " 'asus',\n",
       " 'atc',\n",
       " 'atct',\n",
       " 'athlete',\n",
       " 'atl',\n",
       " 'atlanta',\n",
       " 'atlantic',\n",
       " 'atrocious',\n",
       " 'attach',\n",
       " 'attached',\n",
       " 'attackingbabymomma',\n",
       " 'attdt',\n",
       " 'attempt',\n",
       " 'attempted',\n",
       " 'attempting',\n",
       " 'attend',\n",
       " 'attendant',\n",
       " 'attended',\n",
       " 'attendee',\n",
       " 'attendent',\n",
       " 'attendents',\n",
       " 'attending',\n",
       " 'attention',\n",
       " 'attentiveness',\n",
       " 'attitude',\n",
       " 'attndt',\n",
       " 'atx',\n",
       " 'au',\n",
       " 'auciello',\n",
       " 'auction',\n",
       " 'audio',\n",
       " 'auf',\n",
       " 'august',\n",
       " 'auh',\n",
       " 'aussie',\n",
       " 'austin',\n",
       " 'austinairport',\n",
       " 'australia',\n",
       " 'austrian',\n",
       " 'author',\n",
       " 'authorize',\n",
       " 'auto',\n",
       " 'automated',\n",
       " 'automatic',\n",
       " 'automatically',\n",
       " 'avail',\n",
       " 'availability',\n",
       " 'available',\n",
       " 'avalonhollywood',\n",
       " 'avatar',\n",
       " 'average',\n",
       " 'averted',\n",
       " 'avgeek',\n",
       " 'aviation',\n",
       " 'aviv',\n",
       " 'avoid',\n",
       " 'avoided',\n",
       " 'avon',\n",
       " 'aw',\n",
       " 'await',\n",
       " 'awaiting',\n",
       " 'awake',\n",
       " 'award',\n",
       " 'aware',\n",
       " 'away',\n",
       " 'awe',\n",
       " 'aweful',\n",
       " 'awesome',\n",
       " 'awesomeness',\n",
       " 'awful',\n",
       " 'awhile',\n",
       " 'awkward',\n",
       " 'awrd',\n",
       " 'aww',\n",
       " 'awww',\n",
       " 'awwweesssooomee',\n",
       " 'aye',\n",
       " 'ayyy',\n",
       " 'az',\n",
       " 'ba',\n",
       " 'baby',\n",
       " 'babyfood',\n",
       " 'back',\n",
       " 'backhome',\n",
       " 'backlog',\n",
       " 'backpack',\n",
       " 'backtowinter',\n",
       " 'backup',\n",
       " 'backyard',\n",
       " 'bad',\n",
       " 'badairline',\n",
       " 'badcustomerexperience',\n",
       " 'badcustomerservice',\n",
       " 'bademployeeproblem',\n",
       " 'badge',\n",
       " 'badly',\n",
       " 'badmgmt',\n",
       " 'badpolicy',\n",
       " 'badservice',\n",
       " 'bae',\n",
       " 'bag',\n",
       " 'bagage',\n",
       " 'baggage',\n",
       " 'baggagedrama',\n",
       " 'baggagefail',\n",
       " 'bagsflyfree',\n",
       " 'bagsflyfreebutnotwithme',\n",
       " 'bagtag',\n",
       " 'bahamas',\n",
       " 'bailey',\n",
       " 'baitandswitch',\n",
       " 'baking',\n",
       " 'balance',\n",
       " 'baldordash',\n",
       " 'ball',\n",
       " 'ballbag',\n",
       " 'balloon',\n",
       " 'baltimore',\n",
       " 'band',\n",
       " 'bank',\n",
       " 'bankrupt',\n",
       " 'bankruptcy',\n",
       " 'banning',\n",
       " 'bar',\n",
       " 'barbara',\n",
       " 'barclays',\n",
       " 'barely',\n",
       " 'barnum',\n",
       " 'barrel',\n",
       " 'barrettkarabis',\n",
       " 'base',\n",
       " 'based',\n",
       " 'basic',\n",
       " 'basically',\n",
       " 'basket',\n",
       " 'basketball',\n",
       " 'bass',\n",
       " 'bathroom',\n",
       " 'battery',\n",
       " 'battierccipuppy',\n",
       " 'batting',\n",
       " 'battle',\n",
       " 'battling',\n",
       " 'bay',\n",
       " 'bc',\n",
       " 'bcz',\n",
       " 'bd',\n",
       " 'bday',\n",
       " 'bdl',\n",
       " 'beach',\n",
       " 'bean',\n",
       " 'beanie',\n",
       " 'beantownmatty',\n",
       " 'beat',\n",
       " 'beatriz',\n",
       " 'beautiful',\n",
       " 'beautifull',\n",
       " 'beautifully',\n",
       " 'beauty',\n",
       " 'bebetter',\n",
       " 'become',\n",
       " 'becomes',\n",
       " 'becoming',\n",
       " 'becuase',\n",
       " 'bed',\n",
       " 'beer',\n",
       " 'befor',\n",
       " 'began',\n",
       " 'begging',\n",
       " 'begin',\n",
       " 'beginning',\n",
       " 'begrudgingly',\n",
       " 'behave',\n",
       " 'behind',\n",
       " 'beijing',\n",
       " 'bein',\n",
       " 'beingsuckontarmacsucks',\n",
       " 'belabor',\n",
       " 'believable',\n",
       " 'believe',\n",
       " 'believing',\n",
       " 'belize',\n",
       " 'belligerent',\n",
       " 'belonging',\n",
       " 'beloved',\n",
       " 'belt',\n",
       " 'ben',\n",
       " 'beneficial',\n",
       " 'benefit',\n",
       " 'benes',\n",
       " 'benjaminokeefe',\n",
       " 'beought',\n",
       " 'bereavement',\n",
       " 'bergstrom',\n",
       " 'berlin',\n",
       " 'bernhardtjh',\n",
       " 'besides',\n",
       " 'best',\n",
       " 'bestairline',\n",
       " 'bestairlineever',\n",
       " 'bestemployees',\n",
       " 'bestflightever',\n",
       " 'bestfriend',\n",
       " 'bestinclass',\n",
       " 'bet',\n",
       " 'betch',\n",
       " 'better',\n",
       " 'betty',\n",
       " 'beware',\n",
       " 'beyond',\n",
       " 'bf',\n",
       " 'bff',\n",
       " 'bfs',\n",
       " 'bgm',\n",
       " 'bham',\n",
       " 'bhm',\n",
       " 'bible',\n",
       " 'biceps',\n",
       " 'bicycle',\n",
       " 'big',\n",
       " 'bigger',\n",
       " 'biggest',\n",
       " 'biggie',\n",
       " 'bike',\n",
       " 'bil',\n",
       " 'bila',\n",
       " 'billing',\n",
       " 'billion',\n",
       " 'bin',\n",
       " 'bio',\n",
       " 'bird',\n",
       " 'birmingham',\n",
       " 'birth',\n",
       " 'birthdate',\n",
       " 'birthday',\n",
       " 'bit',\n",
       " 'bitch',\n",
       " 'bite',\n",
       " 'biz',\n",
       " 'bizarre',\n",
       " 'bizness',\n",
       " 'biztravel',\n",
       " 'black',\n",
       " 'blacked',\n",
       " 'blacklist',\n",
       " 'blah',\n",
       " 'blame',\n",
       " 'blameshiftoverload',\n",
       " 'blaming',\n",
       " 'blanc',\n",
       " 'blanket',\n",
       " 'blast',\n",
       " 'blasting',\n",
       " 'blatant',\n",
       " 'blatimore',\n",
       " 'blazer',\n",
       " 'bleed',\n",
       " 'blegh',\n",
       " 'bleh',\n",
       " 'blessed',\n",
       " 'blew',\n",
       " 'blindsided',\n",
       " 'blizzard',\n",
       " 'blocked',\n",
       " 'blocking',\n",
       " 'blog',\n",
       " 'bloody',\n",
       " 'bloodymary',\n",
       " 'bloombergnews',\n",
       " 'blow',\n",
       " 'blowing',\n",
       " 'blown',\n",
       " 'blue',\n",
       " 'bluemanity',\n",
       " 'bluetiful',\n",
       " 'blumanity',\n",
       " 'bna',\n",
       " 'bnasnow',\n",
       " 'board',\n",
       " 'boarded',\n",
       " 'boarding',\n",
       " 'boardingpass',\n",
       " 'boat',\n",
       " 'bobwesson',\n",
       " 'boeing',\n",
       " 'boeingairplanes',\n",
       " 'bogota',\n",
       " 'bohnjai',\n",
       " 'bohol',\n",
       " 'boise',\n",
       " 'bold',\n",
       " 'boldflavors',\n",
       " 'bone',\n",
       " 'bonnie',\n",
       " 'bonsinthesky',\n",
       " 'bonus',\n",
       " 'boo',\n",
       " 'book',\n",
       " 'booked',\n",
       " 'booking',\n",
       " 'bookofnegroes',\n",
       " 'bool',\n",
       " 'boom',\n",
       " 'boot',\n",
       " 'bora',\n",
       " 'border',\n",
       " 'bos',\n",
       " 'boson',\n",
       " 'boston',\n",
       " 'bostonbbb',\n",
       " 'bostonlogan',\n",
       " 'bother',\n",
       " 'bothered',\n",
       " 'bottle',\n",
       " 'bought',\n",
       " 'bounce',\n",
       " 'bounced',\n",
       " 'bound',\n",
       " 'bourbon',\n",
       " 'bout',\n",
       " 'box',\n",
       " 'boy',\n",
       " 'boycott',\n",
       " 'boyfriend',\n",
       " 'bpdfpp',\n",
       " 'bqn',\n",
       " 'bqonpa',\n",
       " 'bracket',\n",
       " 'brag',\n",
       " 'bragging',\n",
       " 'brain',\n",
       " 'brand',\n",
       " 'brandloveaffair',\n",
       " 'brandmance',\n",
       " 'brandssayingbae',\n",
       " 'brave',\n",
       " 'braved',\n",
       " 'braving',\n",
       " 'bravo',\n",
       " 'brazil',\n",
       " 'bread',\n",
       " 'break',\n",
       " 'breakfast',\n",
       " 'breastfeeding',\n",
       " 'breath',\n",
       " 'breeze',\n",
       " 'breezy',\n",
       " 'brian',\n",
       " 'brianregancomic',\n",
       " 'bride',\n",
       " 'bridesmaid',\n",
       " 'bridge',\n",
       " 'brilliant',\n",
       " 'bring',\n",
       " 'bringing',\n",
       " 'brings',\n",
       " 'brisk',\n",
       " 'briughy',\n",
       " 'bro',\n",
       " 'broadway',\n",
       " 'broke',\n",
       " 'broken',\n",
       " 'brokenpromises',\n",
       " 'brokenwheel',\n",
       " 'brooklyn',\n",
       " 'brothaaaaaa',\n",
       " 'brother',\n",
       " 'brought',\n",
       " 'browser',\n",
       " 'brrr',\n",
       " 'bruh',\n",
       " 'bruin',\n",
       " 'brush',\n",
       " 'brushing',\n",
       " 'brussels',\n",
       " 'brutal',\n",
       " 'btr',\n",
       " 'bttr',\n",
       " 'btv',\n",
       " 'btw',\n",
       " 'buck',\n",
       " 'bucket',\n",
       " 'buddy',\n",
       " 'budget',\n",
       " 'buenos',\n",
       " 'buf',\n",
       " 'buffalo',\n",
       " 'bug',\n",
       " 'bugging',\n",
       " 'built',\n",
       " 'buisness',\n",
       " 'bulkhead',\n",
       " 'bull',\n",
       " 'bullshit',\n",
       " 'bummed',\n",
       " 'bummer',\n",
       " 'bump',\n",
       " 'bumped',\n",
       " 'bumper',\n",
       " 'bumping',\n",
       " 'bunch',\n",
       " 'bundle',\n",
       " 'bur',\n",
       " 'burden',\n",
       " 'burgundy',\n",
       " 'burlington',\n",
       " 'burn',\n",
       " 'burned',\n",
       " 'burningman',\n",
       " 'burrito',\n",
       " 'burst',\n",
       " 'bus',\n",
       " 'bush',\n",
       " 'business',\n",
       " 'bussey',\n",
       " 'busted',\n",
       " 'busy',\n",
       " 'butt',\n",
       " 'button',\n",
       " 'buy',\n",
       " 'buyback',\n",
       " 'buyer',\n",
       " 'buying',\n",
       " 'bwi',\n",
       " 'bwood',\n",
       " 'bypass',\n",
       " 'bze',\n",
       " 'ca',\n",
       " 'cab',\n",
       " 'cabaret',\n",
       " 'cabcelled',\n",
       " 'cabin',\n",
       " 'cac',\n",
       " 'cache',\n",
       " 'caching',\n",
       " 'cae',\n",
       " 'caexhibitions',\n",
       " 'caffeine',\n",
       " 'cak',\n",
       " 'cake',\n",
       " 'cal',\n",
       " 'calendar',\n",
       " 'calgary',\n",
       " 'cali',\n",
       " 'calibrated',\n",
       " 'california',\n",
       " 'call',\n",
       " 'callback',\n",
       " 'called',\n",
       " 'caller',\n",
       " 'calling',\n",
       " 'calming',\n",
       " 'came',\n",
       " 'camera',\n",
       " 'campaign',\n",
       " 'canada',\n",
       " 'cancelled',\n",
       " 'cancer',\n",
       " 'cancun',\n",
       " 'candace',\n",
       " 'caned',\n",
       " 'canned',\n",
       " 'cant',\n",
       " 'canthurtasking',\n",
       " 'cantlogoutofunitedwifi',\n",
       " 'canx',\n",
       " 'cap',\n",
       " 'capability',\n",
       " 'capacity',\n",
       " 'capt',\n",
       " 'captain',\n",
       " 'captive',\n",
       " 'car',\n",
       " 'caravannyc',\n",
       " 'card',\n",
       " 'care',\n",
       " 'cared',\n",
       " 'career',\n",
       " 'careyon',\n",
       " 'cargo',\n",
       " 'caring',\n",
       " 'carmen',\n",
       " 'carolina',\n",
       " 'carousel',\n",
       " 'carrie',\n",
       " 'carried',\n",
       " 'carrier',\n",
       " 'carrieunderwood',\n",
       " 'carry',\n",
       " 'carrying',\n",
       " 'carryon',\n",
       " 'carryonbagssloweverybodydown',\n",
       " 'carryons',\n",
       " 'carseat',\n",
       " 'cart',\n",
       " 'cartagena',\n",
       " 'case',\n",
       " 'cash',\n",
       " 'casimir',\n",
       " 'catch',\n",
       " 'catching',\n",
       " 'catering',\n",
       " 'cattle',\n",
       " 'caught',\n",
       " 'cause',\n",
       " 'caused',\n",
       " 'causing',\n",
       " 'caution',\n",
       " 'cb',\n",
       " 'cbcallinaday',\n",
       " 'cbcnews',\n",
       " 'cbsnews',\n",
       " 'cbsphilly',\n",
       " 'cc',\n",
       " 'cdg',\n",
       " 'cdn',\n",
       " 'cebu',\n",
       " 'celebrate',\n",
       " 'celebrates',\n",
       " 'celebrating',\n",
       " 'celebs',\n",
       " 'cell',\n",
       " 'cellphone',\n",
       " 'cent',\n",
       " 'center',\n",
       " 'central',\n",
       " 'centre',\n",
       " 'century',\n",
       " 'ceo',\n",
       " 'cert',\n",
       " 'certain',\n",
       " 'certainly',\n",
       " 'certificate',\n",
       " 'cessnas',\n",
       " 'cgjase',\n",
       " 'cgroup',\n",
       " 'cha',\n",
       " 'chaching',\n",
       " 'chair',\n",
       " 'chairman',\n",
       " 'chairmanlove',\n",
       " 'chalk',\n",
       " 'challenge',\n",
       " 'challenging',\n",
       " 'champagne',\n",
       " 'championship',\n",
       " 'chance',\n",
       " 'change',\n",
       " 'changed',\n",
       " 'changer',\n",
       " 'changing',\n",
       " 'channel',\n",
       " 'chaos',\n",
       " 'chaotic',\n",
       " 'chapter',\n",
       " 'character',\n",
       " 'charge',\n",
       " 'charged',\n",
       " 'charger',\n",
       " 'charging',\n",
       " 'charity',\n",
       " 'charleston',\n",
       " 'charlotte',\n",
       " 'charm',\n",
       " 'charter',\n",
       " 'chartering',\n",
       " 'chase',\n",
       " 'chasefoster',\n",
       " 'chat',\n",
       " 'chatting',\n",
       " 'cheap',\n",
       " 'cheaper',\n",
       " 'cheapest',\n",
       " 'cheapflights',\n",
       " 'cheapoair',\n",
       " 'cheapoairchat',\n",
       " 'cheapslogannotmotto',\n",
       " 'cheatcustomers',\n",
       " 'cheated',\n",
       " 'check',\n",
       " 'checked',\n",
       " 'checker',\n",
       " 'checkin',\n",
       " 'checking',\n",
       " 'checkout',\n",
       " 'checkpoint',\n",
       " 'cheer',\n",
       " 'cheertymedad',\n",
       " 'cheerupdates',\n",
       " 'cheese',\n",
       " 'cheesy',\n",
       " 'cheeze',\n",
       " 'chef',\n",
       " 'chemistry',\n",
       " 'cherry',\n",
       " 'chewey',\n",
       " 'chg',\n",
       " 'chging',\n",
       " 'chi',\n",
       " 'chicago',\n",
       " 'chicagotribune',\n",
       " 'chicken',\n",
       " 'chief',\n",
       " 'child',\n",
       " 'childish',\n",
       " 'childrens',\n",
       " 'chill',\n",
       " 'chillpill',\n",
       " 'chilly',\n",
       " 'china',\n",
       " 'chinese',\n",
       " 'chip',\n",
       " 'chk',\n",
       " 'chkd',\n",
       " 'chkin',\n",
       " ...]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f7c19a79b9aeb8b1cc18eda6778f62d726c8b19540b84d23ad80114035b2e0b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
