{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text_Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text= \"Awesome!!!, This is not  fantastic!!!. We are very pleased...3456\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "\n",
    "def cleaning_text(data):\n",
    "    text_token = word_tokenize(data.lower())   #datayi kücük harfe cevirip, kelimelere ayiriyor ve liste formatinda siraliyor\n",
    "\n",
    "    tokens_without_punct = [w for w in text_token if w.isalpha()] #noktalama özel karekterler ve sayilari siler\n",
    "\n",
    "    tokens_without_stopsword = [w for w in tokens_without_punct if w not in stopwords] #dtopswordlerden arindirir\n",
    "\n",
    "    text_cleaned = [WordNetLemmatizer().lemmatize(w) for w in tokens_without_stopsword]  #kelimeleri kök haline getirir\n",
    "\n",
    "    return ' '.join(text_cleaned)  # sonra bunlari text olarak birlestirip bize yeni bir text olarak sunar\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "cleaning_text(sample_text)\n",
    "#yada \n",
    "\n",
    "pd.Series(sample_text).apply(cleaning_text)    \n",
    "\n",
    "\n",
    "# Text temizleme icin yapilan islemler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_without_punc = [w for w in word_token if w.isalpha()]  # .isalnum() for number and object\n",
    "tokens_without_punc  # noktalama özel karekter ve sayilar silinir. ,eğer sayılar kalması istenirse isalnum() kullanılır\n",
    "\n",
    "#text'deki tüm karakterler alfabetikse ve text'de en az bir karakter varsa True, aksi halde False döndürün.bir tane bosluk, veya sayi varsa,False döndürür\n",
    "\n",
    "# Output : ['awesome', 'this', 'is', 'fantastic', 'we', 'are', 'very', 'pleased']  sayilarida siliyor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_without_punc = [w for w in word_token if w.isalnum()] # .isalnum() for number and object\n",
    "tokens_without_punc  # noktalama özel karekter  silinir. ,eğer sayılar kalması istenirse isalnum() kullanılır\n",
    "\n",
    "# eger kelimeler ve sayilarida almak istersek isalnum kullaniyoruz,bosluk veya noktalama @ isretleri varsa False verir sayi ve harf olmali\n",
    "\n",
    "#alpha numeric kisaltmasi\n",
    "\n",
    "# Output: ['awesome', 'this', 'is', 'fantastic', 'we', 'are', 'very', 'pleased', '3456']  sayilari birakiyor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  cümle analizi yapılacak ise olumsuzluk fiil yardımcı fiiller silinmez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem = [WordNetLemmatizer().lemmatize(t,pos = 'v') for t in token_without_sw]\n",
    "lem\n",
    "\n",
    "#burda pos ='v' yaptigimiz icin fiilin kök halini aldi please aldi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" \".join(lem) #liste halindeki token degiskeninin adini\n",
    "#burda yukarda´kileri bosluga join ettik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Stopwords\n",
    "for i in [\"not\", \"no\"]:    #yardimci fiileri kaldirmak istemiyoruz, onlara eklenen no,not larin kalmasini istiyoruz\n",
    "    stop_words.remove(i)\n",
    "\n",
    "def cleaning_text_for_sentiment_analyses(data):\n",
    "    \n",
    "    \n",
    "    #1. removing upper brackets to keep negative auxiliary verbs in text\n",
    "    #yardimci fiilleri metinde tutmak icin tirnak isaretini kaldirmak\n",
    "    text = data.replace(\"'\",'')\n",
    "         \n",
    "    #2. Tokenize\n",
    "    text_tokens = word_tokenize(text.lower()) \n",
    "    \n",
    "    #3. Remove numbers\n",
    "    tokens_without_punc = [w for w in text_tokens if w.isalpha()]  #noktalama isaretlerini siliyor,sayilarida siliyor\n",
    "    \n",
    "    \n",
    "        \n",
    "    tokens_without_sw = [t for t in tokens_without_punc if t not in stop_words]\n",
    "    \n",
    "    #4. lemma\n",
    "    text_cleaned = [WordNetLemmatizer().lemmatize(t) for t in tokens_without_sw]\n",
    "    \n",
    "    #joining\n",
    "    return \" \".join(text_cleaned)\n",
    "\n",
    "# bu islemler sentiment analyses yapinca datayi temizliyoruz. bu burda icinde no not kelimeleri haric tutuluyor, cünkü sentiment yaparken bunlar önemli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f7c19a79b9aeb8b1cc18eda6778f62d726c8b19540b84d23ad80114035b2e0b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
