{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text_Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text= \"Awesome!!!, This is not  fantastic!!!. We are very pleased...3456\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "\n",
    "def cleaning_text(data):\n",
    "    text_token = word_tokenize(data.lower())   #datayi kücük harfe cevirip, kelimelere ayiriyor ve liste formatinda siraliyor\n",
    "\n",
    "    tokens_without_punct = [w for w in text_token if w.isalpha()] #noktalama özel karekterler ve sayilari siler\n",
    "\n",
    "    tokens_without_stopsword = [w for w in tokens_without_punct if w not in stopwords] #dtopswordlerden arindirir\n",
    "\n",
    "    text_cleaned = [WordNetLemmatizer().lemmatize(w) for w in tokens_without_stopsword]  #kelimeleri kök haline getirir\n",
    "\n",
    "    return ' '.join(text_cleaned)  # sonra bunlari text olarak birlestirip bize yeni bir text olarak sunar\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "cleaning_text(sample_text)\n",
    "#yada \n",
    "\n",
    "pd.Series(sample_text).apply(cleaning_text)    \n",
    "\n",
    "\n",
    "# Text temizleme icin yapilan islemler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_without_punc = [w for w in word_token if w.isalpha()]  # .isalnum() for number and object\n",
    "tokens_without_punc  # noktalama özel karekter ve sayilar silinir. ,eğer sayılar kalması istenirse isalnum() kullanılır\n",
    "\n",
    "#text'deki tüm karakterler alfabetikse ve text'de en az bir karakter varsa True, aksi halde False döndürün.bir tane bosluk, veya sayi varsa,False döndürür\n",
    "\n",
    "# Output : ['awesome', 'this', 'is', 'fantastic', 'we', 'are', 'very', 'pleased']  sayilarida siliyor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_without_punc = [w for w in word_token if w.isalnum()] # .isalnum() for number and object\n",
    "tokens_without_punc  # noktalama özel karekter  silinir. ,eğer sayılar kalması istenirse isalnum() kullanılır\n",
    "\n",
    "# eger kelimeler ve sayilarida almak istersek isalnum kullaniyoruz,bosluk veya noktalama @ isretleri varsa False verir sayi ve harf olmali\n",
    "\n",
    "#alpha numeric kisaltmasi\n",
    "\n",
    "# Output: ['awesome', 'this', 'is', 'fantastic', 'we', 'are', 'very', 'pleased', '3456']  sayilari birakiyor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  cümle analizi yapılacak ise olumsuzluk fiil yardımcı fiiller silinmez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem = [WordNetLemmatizer().lemmatize(t,pos = 'v') for t in token_without_sw]\n",
    "lem\n",
    "\n",
    "#burda pos ='v' yaptigimiz icin fiilin kök halini aldi please aldi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" \".join(lem) #liste halindeki token degiskeninin adini\n",
    "#burda yukarda´kileri bosluga join ettik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Stopwords\n",
    "for i in [\"not\", \"no\"]:    #yardimci fiileri kaldirmak istemiyoruz, onlara eklenen no,not larin kalmasini istiyoruz\n",
    "    stop_words.remove(i)\n",
    "\n",
    "def cleaning_text_for_sentiment_analyses(data):\n",
    "    \n",
    "    \n",
    "    #1. removing upper brackets to keep negative auxiliary verbs in text\n",
    "    #yardimci fiilleri metinde tutmak icin tirnak isaretini kaldirmak\n",
    "    text = data.replace(\"'\",'')\n",
    "         \n",
    "    #2. Tokenize\n",
    "    text_tokens = word_tokenize(text.lower()) \n",
    "    \n",
    "    #3. Remove numbers\n",
    "    tokens_without_punc = [w for w in text_tokens if w.isalpha()]  #noktalama isaretlerini siliyor,sayilarida siliyor\n",
    "    \n",
    "    \n",
    "        \n",
    "    tokens_without_sw = [t for t in tokens_without_punc if t not in stop_words]\n",
    "    \n",
    "    #4. lemma\n",
    "    text_cleaned = [WordNetLemmatizer().lemmatize(t) for t in tokens_without_sw]\n",
    "    \n",
    "    #joining\n",
    "    return \" \".join(text_cleaned)\n",
    "\n",
    "# bu islemler sentiment analyses yapinca datayi temizliyoruz. bu burda icinde no not kelimeleri haric tutuluyor, cünkü sentiment yaparken bunlar önemli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd   # textleri tablo halinde göstermek ve CSV dosyalarini kullanmak icin\n",
    "from textblob import TextBlob  # https://textblob.readthedocs.io/en/dev/index.html\n",
    "import streamlit as st\n",
    "import cleantext\n",
    "\n",
    "st.header('The-Rugs Sentiment Analysis')\n",
    "\n",
    "\n",
    "with st.expander('Analyze Text'):\n",
    "    text = st.text_input('Text here please')\n",
    "    if text:   #burda yani o expander icinde bir metin varsa, yani bos degilse, anlamina geliyor,biseler olunca demek\n",
    "        blob = TextBlob(text)  #girilen metni bu hazir library degerlendiriyor\n",
    "        st.write('Polarity: ', round(blob.sentiment.polarity, 2))  #polarity library icinde hazir hesaplanan degeri alacagiz.pretrained edilmis bir kütüphane\n",
    "        st.write('Subjectivity: ', round(blob.sentiment.subjectivity, 2))\n",
    "        def output_sentiment():\n",
    "            if round(blob.sentiment.polarity, 2) > 0.5:\n",
    "                output_positive = 'Positive'\n",
    "                return output_positive\n",
    "            else:\n",
    "                output_negative = 'Negative'\n",
    "                return output_negative\n",
    "        st.write('Sentiment : ', output_sentiment())  #bu fonksiyon olarak koydum, ve ona sira geldiginde hemen calismasi icin\n",
    "\n",
    "    pre = st.text_input('Clean Text : ')\n",
    "    if pre:\n",
    "        st.write(cleantext.clean(pre, clean_all=False, extra_spaces=True, stopwords= False, lowercase=True,\n",
    "                                 numbers=True, punct=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd   # textleri tablo halinde göstermek ve CSV dosyalarini kullanmak icin\n",
    "from textblob import TextBlob  # https://textblob.readthedocs.io/en/dev/index.html\n",
    "import streamlit as st\n",
    "import cleantext\n",
    "\n",
    "\n",
    "\n",
    "st.header('The-Rugs Sentiment Analysis')\n",
    "\n",
    "\n",
    "with st.expander('Analyze Text'):\n",
    "    text = st.text_input('Text here please')\n",
    "    if text:   #burda yani o expander icinde bir metin varsa, yani bos degilse, anlamina geliyor,biseler olunca demek\n",
    "        blob = TextBlob(text)  #girilen metni bu hazir library degerlendiriyor\n",
    "        st.write('Polarity: ', round(blob.sentiment.polarity, 2))  #polarity library icinde hazir hesaplanan degeri alacagiz.pretrained edilmis bir kütüphane\n",
    "        st.write('Subjectivity: ', round(blob.sentiment.subjectivity, 2))\n",
    "        def output_sentiment():\n",
    "            if round(blob.sentiment.polarity, 2) > 0.5:\n",
    "                output_positive = 'Positive'\n",
    "                return output_positive\n",
    "            else:\n",
    "                output_negative = 'Negative'\n",
    "                return output_negative\n",
    "        st.write('Sentiment : ', output_sentiment())  #bu fonksiyon olarak koydum, ve ona sira geldiginde hemen calismasi icin\n",
    "\n",
    "    pre = st.text_input('Clean Text : ')\n",
    "    if pre:\n",
    "        st.write(cleantext.clean(pre, clean_all=False, extra_spaces=True, stopwords= True, lowercase=True,\n",
    "                                 numbers=True, punct=True))   #numaralari ve punctuation silmek icin, burda stopwordleri silerken not olanlari silmiyor,denedim\n",
    "\n",
    "with st.expander('Analyze The Entire File'):\n",
    "    upl = st.file_uploader('Upload File')\n",
    "\n",
    "    def score(x):\n",
    "        blob1 = TextBlob(x)\n",
    "        return blob1.sentiment.polarity\n",
    "\n",
    "\n",
    "\n",
    "    def analyze(x):\n",
    "        if x >= 0.5:\n",
    "            return 'Positive'\n",
    "        elif x<= -0.5:\n",
    "            return 'Negative'\n",
    "        else:\n",
    "            return 'Neutral'\n",
    "\n",
    "\n",
    "    if upl:\n",
    "        df = pd.read_excel(upl)\n",
    "        #del df['Unnamed: 0']\n",
    "        df['score'] = df['COMMENTS'].apply(score)               #bu sütunu ben kendim yeni olusturuyorum, icindeki score ise yukarida tanimladigim bir function\n",
    "        df['analyze'] = df['score'].apply(analyze)              # bunuda ben yeni olusturuyorum\n",
    "        st.write(df)                                            #tüm yorumlari görmek istedigim icin df.head(10) gibi bir sinirlama yapmadim\n",
    "\n",
    "\n",
    "    #bu okuma islemi her bir comment'te calisacak, ve herbirini okuyacak\n",
    "\n",
    "\n",
    "        @st.cache  # bu sekilde eger bu dosyayi bir daha okutmak istersen, öncekileri hafizasinda turuyor, ve yenileri aliyor\n",
    "                   # bu sekilde daha az maliyetli bir calisma olur\n",
    "        def convert_df(df):\n",
    "\n",
    "            return df.to_csv().encode('utf-8')\n",
    "\n",
    "        csv = convert_df(df)\n",
    "\n",
    "\n",
    "        st.download_button(\n",
    "            label='Download data as CSV',\n",
    "            data = csv,\n",
    "            file_name='Sentmiment.csv',\n",
    "            mime='text/csv'\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd                                                             # textleri tablo halinde göstermek ve CSV dosyalarini kullanmak icin\n",
    "from textblob import TextBlob                                                   # https://textblob.readthedocs.io/en/dev/index.html\n",
    "import streamlit as st\n",
    "import cleantext\n",
    "\n",
    "\n",
    "\n",
    "st.header('The-Rugs Sentiment Analysis')\n",
    "\n",
    "\n",
    "with st.expander('Analyze Text'):\n",
    "    text = st.text_input('Text here please')                                     # buda bir alt satir olusturuyor ve icine yapmasi gerekenleri söylüyorsun\n",
    "    if text:                                                                     # burda yani o expander icinde bir metin varsa, yani bos degilse, anlamina geliyor,biseler olunca demek\n",
    "        blob = TextBlob(text)                                                    # girilen metni bu hazir library degerlendiriyor\n",
    "        st.write('Polarity: ', round(blob.sentiment.polarity, 2))                # polarity library icinde hazir hesaplanan degeri alacagiz.pretrained edilmis bir kütüphane\n",
    "        st.write('Subjectivity: ', round(blob.sentiment.subjectivity, 2))\n",
    "        def output_sentiment():\n",
    "            if round(blob.sentiment.polarity, 2) > 0.5:\n",
    "                output_positive = 'Positive'\n",
    "                return output_positive\n",
    "            else:\n",
    "                output_negative = 'Negative'\n",
    "                return output_negative\n",
    "        st.write('Sentiment : ', output_sentiment())                             # bu fonksiyon olarak koydum, ve ona sira geldiginde hemen calismasi icin\n",
    "\n",
    "    pre = st.text_input('Clean Text : ')                                         # bu da orda bir satir acti, su an biz expander Analyze Text altindayiz\n",
    "    if pre:\n",
    "        st.write(cleantext.clean(pre, clean_all=False, extra_spaces=True, stopwords= True, lowercase=True,\n",
    "                                 numbers=True, punct=True))                      # numaralari ve punctuation silmek icin, burda stopwordleri silerken not olanlari silmiyor,denedim\n",
    "\n",
    "with st.expander('Analyze The Entire File'):\n",
    "    upl = st.file_uploader('Upload File')                                        # bunun ile dosyayi yüklemek icin pencere aciliyor, ve dosyayi secme islemi yapiliyor\n",
    "\n",
    "    def score(x):\n",
    "        blob1 = TextBlob(x)\n",
    "        return blob1.sentiment.polarity\n",
    "\n",
    "\n",
    "\n",
    "    def analyze(x):\n",
    "        if x >= 0.5:\n",
    "            return 'Positive'\n",
    "        elif x<= -0.5:\n",
    "            return 'Negative'\n",
    "        else:\n",
    "            return 'Neutral'\n",
    "\n",
    "\n",
    "    if upl:\n",
    "        df = pd.read_excel(upl)\n",
    "        #del df['Unnamed: 0']\n",
    "        df['score'] = df['COMMENTS'].apply(score)               #bu sütunu ben kendim yeni olusturuyorum, icindeki score ise yukarida tanimladigim bir function\n",
    "        df['analyze'] = df['score'].apply(analyze)              # bunuda ben yeni olusturuyorum\n",
    "        st.write(df)                                            #tüm yorumlari görmek istedigim icin df.head(10) gibi bir sinirlama yapmadim\n",
    "\n",
    "    if upl:\n",
    "        df = pd.read_excel(upl)\n",
    "        df['score'] = df['COMMENTS'].apply(score)\n",
    "        df['analyze'] = df['score'].apply(analyze)\n",
    "        st.write(df)\n",
    "\n",
    "        #bu okuma islemi her bir comment'te calisacak, ve herbirini okuyacak\n",
    "\n",
    "\n",
    "        @st.cache                                               # bu sekilde eger bu dosyayi bir daha okutmak istersen, öncekileri hafizasinda turuyor, ve yenileri aliyor\n",
    "                                                                # bu sekilde daha az maliyetli bir calisma olur\n",
    "        def convert_df(df):\n",
    "\n",
    "            return df.to_csv().encode('utf-8')                  #bu sekilde ise, datayi excelden ceviriyor ve csv olarak kaydetmemiz icin\n",
    "\n",
    "        csv = convert_df(df)                                    #burda csv olarak datamizi yakaliyoruz, asagidaki satirlarda data=csv yazacagiz\n",
    "\n",
    "\n",
    "        st.download_button(\n",
    "            label='Download data as CSV',\n",
    "            data = csv,\n",
    "            file_name='Sentmiment.csv',\n",
    "            mime='text/csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [\"not\", \"no\"]:    #yardimci fiileri kaldirmak istemiyoruz, onlara eklenen no,not larin kalmasini istiyoruz\n",
    "    stop_words.remove(i)\n",
    "\n",
    "def cleaning_text_for_sentiment_analyses(data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with st.expander('Analyze The Entire File'):\n",
    "    upl = st.file_uploader('Upload File')                                        # bunun ile dosyayi yüklemek icin pencere aciliyor, ve dosyayi secme islemi yapiliyor\n",
    "\n",
    "    def score(x):\n",
    "        blob1 = TextBlob(x)\n",
    "        return blob1.sentiment.polarity\n",
    "\n",
    "\n",
    "\n",
    "    def analyze(x):\n",
    "        if x >= 0.5:\n",
    "            return 'Positive'\n",
    "        elif x<= -0.5:\n",
    "            return 'Negative'\n",
    "        else:\n",
    "            return 'Neutral'\n",
    "\n",
    "\n",
    "    if upl:\n",
    "        df = pd.read_excel(upl)\n",
    "        #del df['Unnamed: 0']\n",
    "        df['score'] = df['COMMENTS'].apply(score)               #bu sütunu ben kendim yeni olusturuyorum, icindeki score ise yukarida tanimladigim bir function\n",
    "        df['analyze'] = df['score'].apply(analyze)              # bunuda ben yeni olusturuyorum\n",
    "        st.write(df)                                            #tüm yorumlari görmek istedigim icin df.head(10) gibi bir sinirlama yapmadim\n",
    "\n",
    "    if upl:\n",
    "        df = pd.read_excel(upl)\n",
    "        df['score'] = df['COMMENTS'].apply(score)\n",
    "        df['analyze'] = df['score'].apply(analyze)\n",
    "        st.write(df)\n",
    "\n",
    "        #bu okuma islemi her bir comment'te calisacak, ve herbirini okuyacak\n",
    "\n",
    "\n",
    "        @st.cache                                               # bu sekilde eger bu dosyayi bir daha okutmak istersen, öncekileri hafizasinda turuyor, ve yenileri aliyor\n",
    "                                                                # bu sekilde daha az maliyetli bir calisma olur\n",
    "        def convert_df(df):\n",
    "\n",
    "            return df.to_csv().encode('utf-8')                  #bu sekilde ise, datayi excelden ceviriyor ve csv olarak kaydetmemiz icin\n",
    "\n",
    "        csv = convert_df(df)                                    #burda csv olarak datamizi yakaliyoruz, asagidaki satirlarda data=csv yazacagiz\n",
    "\n",
    "\n",
    "        st.download_button(\n",
    "            label='Download data as CSV',\n",
    "            data = csv,\n",
    "            file_name='Sentmiment.csv',\n",
    "            mime='text/csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd                                                             # textleri tablo halinde göstermek ve CSV dosyalarini kullanmak icin\n",
    "from textblob import TextBlob                                                   # https://textblob.readthedocs.io/en/dev/index.html\n",
    "import streamlit as st\n",
    "import cleantext\n",
    "\n",
    "\n",
    "\n",
    "st.header('The-Rugs Sentiment Analysis')\n",
    "\n",
    "\n",
    "with st.expander('Analyze Text'):\n",
    "    text = st.text_input('Text here please')                                     # buda bir alt satir olusturuyor ve icine yapmasi gerekenleri söylüyorsun\n",
    "    if text:                                                                     # burda yani o expander icinde bir metin varsa, yani bos degilse, anlamina geliyor,biseler olunca demek\n",
    "        blob = TextBlob(text)                                                    # girilen metni bu hazir library degerlendiriyor\n",
    "        st.write('Polarity: ', round(blob.sentiment.polarity, 2))                # polarity library icinde hazir hesaplanan degeri alacagiz.pretrained edilmis bir kütüphane\n",
    "        st.write('Subjectivity: ', round(blob.sentiment.subjectivity, 2))\n",
    "        def output_sentiment():\n",
    "            if round(blob.sentiment.polarity, 2) > 0.5:\n",
    "                output_positive = 'Positive'\n",
    "                return output_positive\n",
    "            else:\n",
    "                output_negative = 'Negative'\n",
    "                return output_negative\n",
    "        st.write('Sentiment : ', output_sentiment())                             # bu fonksiyon olarak koydum, ve ona sira geldiginde hemen calismasi icin\n",
    "\n",
    "    pre = st.text_input('Clean Text : ')                                         # bu da orda bir satir acti, su an biz expander Analyze Text altindayiz\n",
    "    if pre:\n",
    "        st.write(cleantext.clean(pre, clean_all=False, extra_spaces=True, stopwords= True, lowercase=True,\n",
    "                                 numbers=True, punct=True))                      # numaralari ve punctuation silmek icin, burda stopwordleri silerken not olanlari silmiyor,denedim\n",
    "\n",
    "with st.expander('Analyze The Entire File'):\n",
    "    upl = st.file_uploader('Upload File')                                        # bunun ile dosyayi yüklemek icin pencere aciliyor, ve dosyayi secme islemi yapiliyor\n",
    "\n",
    "    def score(x):\n",
    "        blob1 = TextBlob(x)\n",
    "        return blob1.sentiment.polarity\n",
    "\n",
    "\n",
    "\n",
    "    def analyze(x):\n",
    "        if x >= 0.5:\n",
    "            return 'Positive'\n",
    "        elif x<= -0.5:\n",
    "            return 'Negative'\n",
    "        else:\n",
    "            return 'Neutral'\n",
    "\n",
    "\n",
    "    if upl:\n",
    "        df = pd.read_excel(upl)\n",
    "        #del df['Unnamed: 0']\n",
    "        df['score'] = df['COMMENTS'].apply(score)               #bu sütunu ben kendim yeni olusturuyorum, icindeki score ise yukarida tanimladigim bir function\n",
    "        df['analyze'] = df['score'].apply(analyze)              # bunuda ben yeni olusturuyorum\n",
    "        st.write(df)                                            #tüm yorumlari görmek istedigim icin df.head(10) gibi bir sinirlama yapmadim\n",
    "\n",
    "    if upl:\n",
    "        df = pd.read_excel(upl)\n",
    "        df['score'] = df['COMMENTS'].apply(score)\n",
    "        df['analyze'] = df['score'].apply(analyze)\n",
    "        st.write(df)\n",
    "\n",
    "        #bu okuma islemi her bir comment'te calisacak, ve herbirini okuyacak\n",
    "\n",
    "\n",
    "        @st.cache                                               # bu sekilde eger bu dosyayi bir daha okutmak istersen, öncekileri hafizasinda turuyor, ve yenileri aliyor\n",
    "                                                                # bu sekilde daha az maliyetli bir calisma olur\n",
    "        def convert_df(df):\n",
    "\n",
    "            return df.to_csv().encode('utf-8')                  #bu sekilde ise, datayi excelden ceviriyor ve csv olarak kaydetmemiz icin\n",
    "\n",
    "        csv = convert_df(df)                                    #burda csv olarak datamizi yakaliyoruz, asagidaki satirlarda data=csv yazacagiz\n",
    "\n",
    "\n",
    "        st.download_button(\n",
    "            label='Download data as CSV',\n",
    "            data = csv,\n",
    "            file_name='Sentmiment.csv',\n",
    "            mime='text/csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd                                                             # textleri tablo halinde göstermek ve CSV dosyalarini kullanmak icin\n",
    "from textblob import TextBlob                                                   # https://textblob.readthedocs.io/en/dev/index.html\n",
    "import streamlit as st\n",
    "import cleantext\n",
    "\n",
    "\n",
    "\n",
    "st.header('The-Rugs Sentiment Analysis')\n",
    "\n",
    "\n",
    "with st.expander('Analyze Text'):\n",
    "    text = st.text_input('Text here please')                                     # buda bir alt satir olusturuyor ve icine yapmasi gerekenleri söylüyorsun\n",
    "    if text:                                                                     # burda yani o expander icinde bir metin varsa, yani bos degilse, anlamina geliyor,biseler olunca demek\n",
    "        blob = TextBlob(text)                                                    # girilen metni bu hazir library degerlendiriyor\n",
    "        st.write('Polarity: ', round(blob.sentiment.polarity, 2))                # polarity library icinde hazir hesaplanan degeri alacagiz.pretrained edilmis bir kütüphane\n",
    "        st.write('Subjectivity: ', round(blob.sentiment.subjectivity, 2))\n",
    "        def output_sentiment():\n",
    "            if round(blob.sentiment.polarity, 2) > 0.5:\n",
    "                output_positive = 'Positive'\n",
    "                return output_positive\n",
    "            else:\n",
    "                output_negative = 'Negative'\n",
    "                return output_negative\n",
    "        st.write('Sentiment : ', output_sentiment())                             # bu fonksiyon olarak koydum, ve ona sira geldiginde hemen calismasi icin\n",
    "\n",
    "    pre = st.text_input('Clean Text : ')                                         # bu da orda bir satir acti, su an biz expander Analyze Text altindayiz\n",
    "    if pre:\n",
    "        st.write(cleantext.clean(pre, clean_all=False, extra_spaces=True, stopwords= True, lowercase=True,\n",
    "                                 numbers=True, punct=True))                      # numaralari ve punctuation silmek icin, burda stopwordleri silerken not olanlari silmiyor,denedim\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "# ANALYZE THE ENTIRE FILE PART\n",
    "\n",
    "with st.expander('Analyze The Entire File'):\n",
    "    uploaded_file = st.file_uploader('Upload File')   # bunun ile dosyayi yüklemek icin pencere aciliyor, ve dosyayi secme islemi yapiliyor\n",
    "    #converting dict to str\n",
    "    conv1 = str(uploaded_file)\n",
    "    split1 = conv1.split(sep=\".\", maxsplit=1)  #maxsplit 1 den fazla bölme islemi yapmasin diye\n",
    "    store1 = split1[1]\n",
    "    #store2 = store1.split(sep=\"'\", maxsplit=-1) # maxsplit=-1 olunca bölme siniri yok, ne kadar varsa o kadar böler\n",
    "    #store2 = store2.split(sep=\".\", maxsplit = 1)\n",
    "    if store1 == 'csv':\n",
    "        st.write('That is a csv File')\n",
    "        df = pd.read_csv(uploaded_file, encoding= 'unicode_escape')\n",
    "        st.write(df)\n",
    "    elif store1 == 'xlsx':\n",
    "        st.write('That is a Excel File')\n",
    "        df = pd.read_excel(uploaded_file)\n",
    "        st.write(df)\n",
    "    else:\n",
    "        st.write('Error: Please upload the file in a csv or excel format')\n",
    "\n",
    "    #Read excel or csv file\n",
    "\n",
    "\n",
    "\n",
    "    def score(x):\n",
    "        blob1 = TextBlob(x)\n",
    "        return blob1.sentiment.polarity\n",
    "\n",
    "\n",
    "\n",
    "    def analyze(x):\n",
    "        if x >= 0.5:\n",
    "            return 'Positive'\n",
    "        elif x<= -0.5:\n",
    "            return 'Negative'\n",
    "        else:\n",
    "            return 'Neutral'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if uploaded_file:\n",
    "        df = pd.read_excel(uploaded_file)\n",
    "        #del df['Unnamed: 0']\n",
    "        df['score'] = df['COMMENTS'].apply(score)               #bu sütunu ben kendim yeni olusturuyorum, icindeki score ise yukarida tanimladigim bir function\n",
    "        df['analyze'] = df['score'].apply(analyze)              # bunuda ben yeni olusturuyorum\n",
    "        st.write(df)                                            #tüm yorumlari görmek istedigim icin df.head(10) gibi bir sinirlama yapmadim\n",
    "\n",
    "\n",
    "                                                                # bu okuma islemi her bir comment'te calisacak, ve herbirini okuyacak\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        @st.cache_data                                              # bu sekilde eger bu dosyayi bir daha okutmak istersen, öncekileri hafizasinda turuyor, ve yenileri aliyor\n",
    "                                                                    # bu sekilde daha az maliyetli bir calisma olur\n",
    "        def convert_df(df):\n",
    "\n",
    "            return df.to_csv().encode('utf-8')                                    #bu sekilde ise, datayi excelden ceviriyor ve csv olarak kaydetmemiz icin\n",
    "\n",
    "        csv = convert_df(df)                                   #burda csv olarak datamizi yakaliyoruz, asagidaki satirlarda data=csv yazacagiz\n",
    "\n",
    "\n",
    "        st.download_button(\n",
    "            label='Download data as CSV',\n",
    "            data = csv,\n",
    "            file_name='Sentmiment.csv',  #buraya excelwriter ile bisey gelmesi gerekiyor\n",
    "            mime='text/csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Okay, now let’s pick a classifier and train it. A good place to start is with a Stochastic Gradient Descent (SGD) classifier, using Scikit-Learn’s SGDClassifier class. This clas‐ sifier has the advantage of being capable of handling very large datasets efficiently. This is in part because SGD deals with training instances independently, one at a time (which also makes SGD well suited for online learning), as we will see later. Let’s create an SGDClassifier and train it on the whole training set:\n",
    "\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier \n",
    "\n",
    "sgd_clf = SGDClassifier(random_state=42)\n",
    "\n",
    "sgd_clf.fit(X_train, y_train_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Otuz Dört\n"
     ]
    }
   ],
   "source": [
    "birler = {\"0\":\"\", \"1\":\"Bir\", \"2\":\"İki\", \"3\":\"Üç\", \"4\":\"Dört\", \"5\":\"Beş\", \"6\":\"Altı\", \"7\":\"Yedi\", \"8\":\"Sekiz\", \"9\":\"Dokuz\"}\n",
    "onlar = {\"0\":\"\", \"1\":\"On\", \"2\":\"Yirmi\", \"3\":\"Otuz\", \"4\":\"Kırk\", \"5\":\"Elli\", \"6\":\"Altmış\", \"7\":\"Yetmiş\", \"8\":\"Seksen\", \"9\":\"Doksan\"}\n",
    "yuzler = {\"0\":\"\", \"1\":\"Yüz\", \"2\":\"İkiyüz\", \"3\":\"Üçyüz\", \"4\":\"Dörtyüz\", \"5\":\"Beşyüz\", \"6\":\"Altıyüz\", \"7\":\"Yediyüz\", \"8\":\"Sekizyüz\", \"9\":\"Dokuzyüz\"}\n",
    "\n",
    "def okunus(sayi):\n",
    "    sayi = str(sayi)\n",
    "    if len(sayi) == 1: return f\"{birler[sayi]}\"\n",
    "    elif len(sayi) == 2: return f\"{onlar[sayi[0]]} {birler[sayi[1]]}\"\n",
    "    elif len(sayi) == 3: return f\"{yuzler[sayi[0]]} {onlar[sayi[1]]} {birler[sayi[2]]}\"\n",
    "\n",
    "sayi = int(input(\"Sayı: \"))\n",
    "print(okunus(sayi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd                                                             # textleri tablo halinde göstermek ve CSV dosyalarini kullanmak icin\n",
    "from textblob import TextBlob                                                   # https://textblob.readthedocs.io/en/dev/index.html\n",
    "import streamlit as st\n",
    "import cleantext\n",
    "\n",
    "\n",
    "\n",
    "st.header('The-Rugs Sentiment Analysis')\n",
    "\n",
    "\n",
    "with st.expander('Analyze Text'):\n",
    "    text = st.text_input('Text here please')                                     # buda bir alt satir olusturuyor ve icine yapmasi gerekenleri söylüyorsun\n",
    "    if text:                                                                     # burda yani o expander icinde bir metin varsa, yani bos degilse, anlamina geliyor,biseler olunca demek\n",
    "        blob = TextBlob(text)                                                    # girilen metni bu hazir library degerlendiriyor\n",
    "        st.write('Polarity: ', round(blob.sentiment.polarity, 2))                # polarity library icinde hazir hesaplanan degeri alacagiz.pretrained edilmis bir kütüphane\n",
    "        st.write('Subjectivity: ', round(blob.sentiment.subjectivity, 2))\n",
    "        def output_sentiment():\n",
    "            if round(blob.sentiment.polarity, 2) > 0.5:\n",
    "                output_positive = 'Positive'\n",
    "                return output_positive\n",
    "            else:\n",
    "                output_negative = 'Negative'\n",
    "                return output_negative\n",
    "        st.write('Sentiment : ', output_sentiment())                             # bu fonksiyon olarak koydum, ve ona sira geldiginde hemen calismasi icin\n",
    "\n",
    "    pre = st.text_input('Clean Text : ')                                         # bu da orda bir satir acti, su an biz expander Analyze Text altindayiz\n",
    "    if pre:\n",
    "        st.write(cleantext.clean(pre, clean_all=False, extra_spaces=True, stopwords= True, lowercase=True,\n",
    "                                 numbers=True, punct=True))                      # numaralari ve punctuation silmek icin, burda stopwordleri silerken not olanlari silmiyor,denedim\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "# ANALYZE THE ENTIRE FILE PART\n",
    "\n",
    "with st.expander('Analyze The Entire File'):\n",
    "    uploaded_file = st.file_uploader('Upload File')   # bunun ile dosyayi yüklemek icin pencere aciliyor, ve dosyayi secme islemi yapiliyor\n",
    "    #converting dict to str\n",
    "    conv1 = str(uploaded_file)\n",
    "    split1 = conv1.split(sep=\".\")  #maxsplit 1 den fazla bölme islemi yapmasin diye\n",
    "    store1 = split1[0]\n",
    "    #store2 = store1.split(sep=\"'\", maxsplit=-1) # maxsplit=-1 olunca bölme siniri yok, ne kadar varsa o kadar böler\n",
    "    #store2 = store2.split(sep=\".\", maxsplit = 1)\n",
    "    if store1 == 'csv':\n",
    "        st.write('That is a csv File')\n",
    "        df = pd.read_csv(uploaded_file, encoding= 'unicode_escape')\n",
    "        st.write(df)\n",
    "    elif store1 == 'xlsx':\n",
    "        st.write('That is a Excel File')\n",
    "        df = pd.read_excel(uploaded_file)\n",
    "        st.write(df)\n",
    "    else:\n",
    "        st.write('Error: Please upload the file in a csv or excel format')\n",
    "\n",
    "    #Read excel or csv file\n",
    "\n",
    "\n",
    "\n",
    "    def score(x):\n",
    "        blob1 = TextBlob(x)\n",
    "        return blob1.sentiment.polarity\n",
    "\n",
    "\n",
    "\n",
    "    def analyze(x):\n",
    "        if x >= 0.5:\n",
    "            return 'Positive'\n",
    "        elif x<= -0.5:\n",
    "            return 'Negative'\n",
    "        else:\n",
    "            return 'Neutral'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #if uploaded_file:\n",
    "        #df = pd.read_excel(uploaded_file)\n",
    "        #del df['Unnamed: 0']\n",
    "        #df['score'] = df['COMMENTS'].apply(score)               #bu sütunu ben kendim yeni olusturuyorum, icindeki score ise yukarida tanimladigim bir function\n",
    "        #df['analyze'] = df['score'].apply(analyze)              # bunuda ben yeni olusturuyorum\n",
    "        #st.write(df)                                            #tüm yorumlari görmek istedigim icin df.head(10) gibi bir sinirlama yapmadim\n",
    "\n",
    "\n",
    "                                                                # bu okuma islemi her bir comment'te calisacak, ve herbirini okuyacak\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        @st.cache_data                                              # bu sekilde eger bu dosyayi bir daha okutmak istersen, öncekileri hafizasinda turuyor, ve yenileri aliyor\n",
    "                                                                    # bu sekilde daha az maliyetli bir calisma olur\n",
    "        def convert_df(df):\n",
    "\n",
    "            return df.to_csv().encode('utf-8')                                    #bu sekilde ise, datayi excelden ceviriyor ve csv olarak kaydetmemiz icin\n",
    "\n",
    "        csv = convert_df(df)                                   #burda csv olarak datamizi yakaliyoruz, asagidaki satirlarda data=csv yazacagiz\n",
    "\n",
    "\n",
    "        st.download_button(\n",
    "            label='Download data as CSV',\n",
    "            data = csv,\n",
    "            file_name='Sentmiment.csv',  #buraya excelwriter ile bisey gelmesi gerekiyor\n",
    "            mime='text/csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd                                                             # textleri tablo halinde göstermek ve CSV dosyalarini kullanmak icin\n",
    "from textblob import TextBlob                                                   # https://textblob.readthedocs.io/en/dev/index.html\n",
    "import streamlit as st\n",
    "import cleantext\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "st.image('the-rugs-logo2.png', use_column_width=True)\n",
    "st.header(':blue[_The-Rugs Sentiment Analysis_]')\n",
    "\n",
    "\n",
    "\n",
    "with st.expander('Analyze Text'):\n",
    "    text = st.text_input('Text here please')                                     # buda bir alt satir olusturuyor ve icine yapmasi gerekenleri söylüyorsun\n",
    "    if text:                                                                     # burda yani o expander icinde bir metin varsa, yani bos degilse, anlamina geliyor,biseler olunca demek\n",
    "        blob = TextBlob(text)                                                    # girilen metni bu hazir library degerlendiriyor\n",
    "        st.write('Polarity: ', round(blob.sentiment.polarity, 2))                # polarity library icinde hazir hesaplanan degeri alacagiz.pretrained edilmis bir kütüphane\n",
    "        st.write('Subjectivity: ', round(blob.sentiment.subjectivity, 2))\n",
    "        def output_sentiment():\n",
    "            if (round(blob.sentiment.polarity, 2)) > 0.5:\n",
    "                output_positive = ':blue[Positive]'\n",
    "                return output_positive\n",
    "            elif (round(blob.sentiment.polarity, 2)) < 0:\n",
    "                output_negative = ':red[Negative]'\n",
    "                return output_negative\n",
    "\n",
    "            else:\n",
    "                output_nötr = 'Nötr'\n",
    "                return output_nötr\n",
    "\n",
    "        st.write('Sentiment : ', output_sentiment())                             # bu fonksiyon olarak koydum, ve ona sira geldiginde hemen calismasi icin\n",
    "\n",
    "    pre = st.text_input('Clean Text : ')                                         # bu da orda bir satir acti, su an biz expander Analyze Text altindayiz\n",
    "    if pre:\n",
    "        st.write(cleantext.clean(pre, clean_all=False, extra_spaces=True, stopwords= True, lowercase=True,\n",
    "                                 numbers=True, punct=True))                      # numaralari ve punctuation silmek icin, burda stopwordleri silerken not olanlari silmiyor,denedim\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "# ANALYZE THE ENTIRE FILE PART\n",
    "\n",
    "with st.expander('Analyze The Entire File'):\n",
    "    uploaded_file = st.file_uploader('Upload File')   # bunun ile dosyayi yüklemek icin pencere aciliyor, ve dosyayi secme islemi yapiliyor\n",
    "\n",
    "\n",
    "    #Read excel or csv file\n",
    "\n",
    "\n",
    "\n",
    "    def score(x):\n",
    "        blob1 = TextBlob(x)\n",
    "        return blob1.sentiment.polarity\n",
    "\n",
    "\n",
    "\n",
    "    def analyze(x):\n",
    "        if x >= 0.5:\n",
    "            return 'Positive'\n",
    "        elif x<= 0:\n",
    "            return 'Negative'\n",
    "        else:\n",
    "            return 'Neutral'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if uploaded_file:\n",
    "        df = pd.read_excel(uploaded_file)\n",
    "        df.dropna(inplace=True)\n",
    "        #del df['Unnamed: 0']\n",
    "        df['score'] = df['COMMENTS'].apply(score)               #bu sütunu ben kendim yeni olusturuyorum, icindeki score ise yukarida tanimladigim bir function\n",
    "        df['analyze'] = df['score'].apply(analyze)              # bunuda ben yeni olusturuyorum\n",
    "        st.write(df)                                            #tüm yorumlari görmek istedigim icin df.head(10) gibi bir sinirlama yapmadim\n",
    "\n",
    "\n",
    "                                                                # bu okuma islemi her bir comment'te calisacak, ve herbirini okuyacak\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        @st.cache_data                                              # bu sekilde eger bu dosyayi bir daha okutmak istersen, öncekileri hafizasinda turuyor, ve yenileri aliyor\n",
    "                                                                    # bu sekilde daha az maliyetli bir calisma olur\n",
    "        def convert_df(df):\n",
    "\n",
    "            return df.to_csv().encode('utf-8')                                    #bu sekilde ise, datayi excelden ceviriyor ve csv olarak kaydetmemiz icin\n",
    "\n",
    "        csv = convert_df(df)                                   #burda csv olarak datamizi yakaliyoruz, asagidaki satirlarda data=csv yazacagiz\n",
    "\n",
    "\n",
    "        st.download_button(\n",
    "            label='Download data as CSV',\n",
    "            data = csv,\n",
    "            file_name='Sentmiment.csv',  #buraya excelwriter ile bisey gelmesi gerekiyor\n",
    "            mime='text/csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f7c19a79b9aeb8b1cc18eda6778f62d726c8b19540b84d23ad80114035b2e0b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
