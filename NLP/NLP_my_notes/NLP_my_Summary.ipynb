{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text_Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text= \"Awesome!!!, This is not  fantastic!!!. We are very pleased...3456\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "\n",
    "def cleaning_text(data):\n",
    "    text_token = word_tokenize(data.lower())   #datayi kücük harfe cevirip, kelimelere ayiriyor ve liste formatinda siraliyor\n",
    "\n",
    "    tokens_without_punct = [w for w in text_token if w.isalpha()] #noktalama özel karekterler ve sayilari siler\n",
    "\n",
    "    tokens_without_stopsword = [w for w in tokens_without_punct if w not in stopwords] #dtopswordlerden arindirir\n",
    "\n",
    "    text_cleaned = [WordNetLemmatizer().lemmatize(w) for w in tokens_without_stopsword]  #kelimeleri kök haline getirir\n",
    "\n",
    "    return ' '.join(text_cleaned)  # sonra bunlari text olarak birlestirip bize yeni bir text olarak sunar\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "cleaning_text(sample_text)\n",
    "#yada \n",
    "\n",
    "pd.Series(sample_text).apply(cleaning_text)    \n",
    "\n",
    "\n",
    "# Text temizleme icin yapilan islemler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_without_punc = [w for w in word_token if w.isalpha()]  # .isalnum() for number and object\n",
    "tokens_without_punc  # noktalama özel karekter ve sayilar silinir. ,eğer sayılar kalması istenirse isalnum() kullanılır\n",
    "\n",
    "#text'deki tüm karakterler alfabetikse ve text'de en az bir karakter varsa True, aksi halde False döndürün.bir tane bosluk, veya sayi varsa,False döndürür\n",
    "\n",
    "# Output : ['awesome', 'this', 'is', 'fantastic', 'we', 'are', 'very', 'pleased']  sayilarida siliyor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_without_punc = [w for w in word_token if w.isalnum()] # .isalnum() for number and object\n",
    "tokens_without_punc  # noktalama özel karekter  silinir. ,eğer sayılar kalması istenirse isalnum() kullanılır\n",
    "\n",
    "# eger kelimeler ve sayilarida almak istersek isalnum kullaniyoruz,bosluk veya noktalama @ isretleri varsa False verir sayi ve harf olmali\n",
    "\n",
    "#alpha numeric kisaltmasi\n",
    "\n",
    "# Output: ['awesome', 'this', 'is', 'fantastic', 'we', 'are', 'very', 'pleased', '3456']  sayilari birakiyor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  cümle analizi yapılacak ise olumsuzluk fiil yardımcı fiiller silinmez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem = [WordNetLemmatizer().lemmatize(t,pos = 'v') for t in token_without_sw]\n",
    "lem\n",
    "\n",
    "#burda pos ='v' yaptigimiz icin fiilin kök halini aldi please aldi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" \".join(lem) #liste halindeki token degiskeninin adini\n",
    "#burda yukarda´kileri bosluga join ettik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Stopwords\n",
    "for i in [\"not\", \"no\"]:    #yardimci fiileri kaldirmak istemiyoruz, onlara eklenen no,not larin kalmasini istiyoruz\n",
    "    stop_words.remove(i)\n",
    "\n",
    "def cleaning_text_for_sentiment_analyses(data):\n",
    "    \n",
    "    \n",
    "    #1. removing upper brackets to keep negative auxiliary verbs in text\n",
    "    #yardimci fiilleri metinde tutmak icin tirnak isaretini kaldirmak\n",
    "    text = data.replace(\"'\",'')\n",
    "         \n",
    "    #2. Tokenize\n",
    "    text_tokens = word_tokenize(text.lower()) \n",
    "    \n",
    "    #3. Remove numbers\n",
    "    tokens_without_punc = [w for w in text_tokens if w.isalpha()]  #noktalama isaretlerini siliyor,sayilarida siliyor\n",
    "    \n",
    "    \n",
    "        \n",
    "    tokens_without_sw = [t for t in tokens_without_punc if t not in stop_words]\n",
    "    \n",
    "    #4. lemma\n",
    "    text_cleaned = [WordNetLemmatizer().lemmatize(t) for t in tokens_without_sw]\n",
    "    \n",
    "    #joining\n",
    "    return \" \".join(text_cleaned)\n",
    "\n",
    "# bu islemler sentiment analyses yapinca datayi temizliyoruz. bu burda icinde no not kelimeleri haric tutuluyor, cünkü sentiment yaparken bunlar önemli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd   # textleri tablo halinde göstermek ve CSV dosyalarini kullanmak icin\n",
    "from textblob import TextBlob  # https://textblob.readthedocs.io/en/dev/index.html\n",
    "import streamlit as st\n",
    "import cleantext\n",
    "\n",
    "st.header('The-Rugs Sentiment Analysis')\n",
    "\n",
    "\n",
    "with st.expander('Analyze Text'):\n",
    "    text = st.text_input('Text here please')\n",
    "    if text:   #burda yani o expander icinde bir metin varsa, yani bos degilse, anlamina geliyor,biseler olunca demek\n",
    "        blob = TextBlob(text)  #girilen metni bu hazir library degerlendiriyor\n",
    "        st.write('Polarity: ', round(blob.sentiment.polarity, 2))  #polarity library icinde hazir hesaplanan degeri alacagiz.pretrained edilmis bir kütüphane\n",
    "        st.write('Subjectivity: ', round(blob.sentiment.subjectivity, 2))\n",
    "        def output_sentiment():\n",
    "            if round(blob.sentiment.polarity, 2) > 0.5:\n",
    "                output_positive = 'Positive'\n",
    "                return output_positive\n",
    "            else:\n",
    "                output_negative = 'Negative'\n",
    "                return output_negative\n",
    "        st.write('Sentiment : ', output_sentiment())  #bu fonksiyon olarak koydum, ve ona sira geldiginde hemen calismasi icin\n",
    "\n",
    "    pre = st.text_input('Clean Text : ')\n",
    "    if pre:\n",
    "        st.write(cleantext.clean(pre, clean_all=False, extra_spaces=True, stopwords= False, lowercase=True,\n",
    "                                 numbers=True, punct=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd   # textleri tablo halinde göstermek ve CSV dosyalarini kullanmak icin\n",
    "from textblob import TextBlob  # https://textblob.readthedocs.io/en/dev/index.html\n",
    "import streamlit as st\n",
    "import cleantext\n",
    "\n",
    "\n",
    "\n",
    "st.header('The-Rugs Sentiment Analysis')\n",
    "\n",
    "\n",
    "with st.expander('Analyze Text'):\n",
    "    text = st.text_input('Text here please')\n",
    "    if text:   #burda yani o expander icinde bir metin varsa, yani bos degilse, anlamina geliyor,biseler olunca demek\n",
    "        blob = TextBlob(text)  #girilen metni bu hazir library degerlendiriyor\n",
    "        st.write('Polarity: ', round(blob.sentiment.polarity, 2))  #polarity library icinde hazir hesaplanan degeri alacagiz.pretrained edilmis bir kütüphane\n",
    "        st.write('Subjectivity: ', round(blob.sentiment.subjectivity, 2))\n",
    "        def output_sentiment():\n",
    "            if round(blob.sentiment.polarity, 2) > 0.5:\n",
    "                output_positive = 'Positive'\n",
    "                return output_positive\n",
    "            else:\n",
    "                output_negative = 'Negative'\n",
    "                return output_negative\n",
    "        st.write('Sentiment : ', output_sentiment())  #bu fonksiyon olarak koydum, ve ona sira geldiginde hemen calismasi icin\n",
    "\n",
    "    pre = st.text_input('Clean Text : ')\n",
    "    if pre:\n",
    "        st.write(cleantext.clean(pre, clean_all=False, extra_spaces=True, stopwords= True, lowercase=True,\n",
    "                                 numbers=True, punct=True))   #numaralari ve punctuation silmek icin, burda stopwordleri silerken not olanlari silmiyor,denedim\n",
    "\n",
    "with st.expander('Analyze The Entire File'):\n",
    "    upl = st.file_uploader('Upload File')\n",
    "\n",
    "    def score(x):\n",
    "        blob1 = TextBlob(x)\n",
    "        return blob1.sentiment.polarity\n",
    "\n",
    "\n",
    "\n",
    "    def analyze(x):\n",
    "        if x >= 0.5:\n",
    "            return 'Positive'\n",
    "        elif x<= -0.5:\n",
    "            return 'Negative'\n",
    "        else:\n",
    "            return 'Neutral'\n",
    "\n",
    "\n",
    "    if upl:\n",
    "        df = pd.read_excel(upl)\n",
    "        #del df['Unnamed: 0']\n",
    "        df['score'] = df['COMMENTS'].apply(score)               #bu sütunu ben kendim yeni olusturuyorum, icindeki score ise yukarida tanimladigim bir function\n",
    "        df['analyze'] = df['score'].apply(analyze)              # bunuda ben yeni olusturuyorum\n",
    "        st.write(df)                                            #tüm yorumlari görmek istedigim icin df.head(10) gibi bir sinirlama yapmadim\n",
    "\n",
    "\n",
    "    #bu okuma islemi her bir comment'te calisacak, ve herbirini okuyacak\n",
    "\n",
    "\n",
    "        @st.cache  # bu sekilde eger bu dosyayi bir daha okutmak istersen, öncekileri hafizasinda turuyor, ve yenileri aliyor\n",
    "                   # bu sekilde daha az maliyetli bir calisma olur\n",
    "        def convert_df(df):\n",
    "\n",
    "            return df.to_csv().encode('utf-8')\n",
    "\n",
    "        csv = convert_df(df)\n",
    "\n",
    "\n",
    "        st.download_button(\n",
    "            label='Download data as CSV',\n",
    "            data = csv,\n",
    "            file_name='Sentmiment.csv',\n",
    "            mime='text/csv'\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd                                                             # textleri tablo halinde göstermek ve CSV dosyalarini kullanmak icin\n",
    "from textblob import TextBlob                                                   # https://textblob.readthedocs.io/en/dev/index.html\n",
    "import streamlit as st\n",
    "import cleantext\n",
    "\n",
    "\n",
    "\n",
    "st.header('The-Rugs Sentiment Analysis')\n",
    "\n",
    "\n",
    "with st.expander('Analyze Text'):\n",
    "    text = st.text_input('Text here please')                                     # buda bir alt satir olusturuyor ve icine yapmasi gerekenleri söylüyorsun\n",
    "    if text:                                                                     # burda yani o expander icinde bir metin varsa, yani bos degilse, anlamina geliyor,biseler olunca demek\n",
    "        blob = TextBlob(text)                                                    # girilen metni bu hazir library degerlendiriyor\n",
    "        st.write('Polarity: ', round(blob.sentiment.polarity, 2))                # polarity library icinde hazir hesaplanan degeri alacagiz.pretrained edilmis bir kütüphane\n",
    "        st.write('Subjectivity: ', round(blob.sentiment.subjectivity, 2))\n",
    "        def output_sentiment():\n",
    "            if round(blob.sentiment.polarity, 2) > 0.5:\n",
    "                output_positive = 'Positive'\n",
    "                return output_positive\n",
    "            else:\n",
    "                output_negative = 'Negative'\n",
    "                return output_negative\n",
    "        st.write('Sentiment : ', output_sentiment())                             # bu fonksiyon olarak koydum, ve ona sira geldiginde hemen calismasi icin\n",
    "\n",
    "    pre = st.text_input('Clean Text : ')                                         # bu da orda bir satir acti, su an biz expander Analyze Text altindayiz\n",
    "    if pre:\n",
    "        st.write(cleantext.clean(pre, clean_all=False, extra_spaces=True, stopwords= True, lowercase=True,\n",
    "                                 numbers=True, punct=True))                      # numaralari ve punctuation silmek icin, burda stopwordleri silerken not olanlari silmiyor,denedim\n",
    "\n",
    "with st.expander('Analyze The Entire File'):\n",
    "    upl = st.file_uploader('Upload File')                                        # bunun ile dosyayi yüklemek icin pencere aciliyor, ve dosyayi secme islemi yapiliyor\n",
    "\n",
    "    def score(x):\n",
    "        blob1 = TextBlob(x)\n",
    "        return blob1.sentiment.polarity\n",
    "\n",
    "\n",
    "\n",
    "    def analyze(x):\n",
    "        if x >= 0.5:\n",
    "            return 'Positive'\n",
    "        elif x<= -0.5:\n",
    "            return 'Negative'\n",
    "        else:\n",
    "            return 'Neutral'\n",
    "\n",
    "\n",
    "    if upl:\n",
    "        df = pd.read_excel(upl)\n",
    "        #del df['Unnamed: 0']\n",
    "        df['score'] = df['COMMENTS'].apply(score)               #bu sütunu ben kendim yeni olusturuyorum, icindeki score ise yukarida tanimladigim bir function\n",
    "        df['analyze'] = df['score'].apply(analyze)              # bunuda ben yeni olusturuyorum\n",
    "        st.write(df)                                            #tüm yorumlari görmek istedigim icin df.head(10) gibi bir sinirlama yapmadim\n",
    "\n",
    "    if upl:\n",
    "        df = pd.read_excel(upl)\n",
    "        df['score'] = df['COMMENTS'].apply(score)\n",
    "        df['analyze'] = df['score'].apply(analyze)\n",
    "        st.write(df)\n",
    "\n",
    "        #bu okuma islemi her bir comment'te calisacak, ve herbirini okuyacak\n",
    "\n",
    "\n",
    "        @st.cache                                               # bu sekilde eger bu dosyayi bir daha okutmak istersen, öncekileri hafizasinda turuyor, ve yenileri aliyor\n",
    "                                                                # bu sekilde daha az maliyetli bir calisma olur\n",
    "        def convert_df(df):\n",
    "\n",
    "            return df.to_csv().encode('utf-8')                  #bu sekilde ise, datayi excelden ceviriyor ve csv olarak kaydetmemiz icin\n",
    "\n",
    "        csv = convert_df(df)                                    #burda csv olarak datamizi yakaliyoruz, asagidaki satirlarda data=csv yazacagiz\n",
    "\n",
    "\n",
    "        st.download_button(\n",
    "            label='Download data as CSV',\n",
    "            data = csv,\n",
    "            file_name='Sentmiment.csv',\n",
    "            mime='text/csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [\"not\", \"no\"]:    #yardimci fiileri kaldirmak istemiyoruz, onlara eklenen no,not larin kalmasini istiyoruz\n",
    "    stop_words.remove(i)\n",
    "\n",
    "def cleaning_text_for_sentiment_analyses(data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with st.expander('Analyze The Entire File'):\n",
    "    upl = st.file_uploader('Upload File')                                        # bunun ile dosyayi yüklemek icin pencere aciliyor, ve dosyayi secme islemi yapiliyor\n",
    "\n",
    "    def score(x):\n",
    "        blob1 = TextBlob(x)\n",
    "        return blob1.sentiment.polarity\n",
    "\n",
    "\n",
    "\n",
    "    def analyze(x):\n",
    "        if x >= 0.5:\n",
    "            return 'Positive'\n",
    "        elif x<= -0.5:\n",
    "            return 'Negative'\n",
    "        else:\n",
    "            return 'Neutral'\n",
    "\n",
    "\n",
    "    if upl:\n",
    "        df = pd.read_excel(upl)\n",
    "        #del df['Unnamed: 0']\n",
    "        df['score'] = df['COMMENTS'].apply(score)               #bu sütunu ben kendim yeni olusturuyorum, icindeki score ise yukarida tanimladigim bir function\n",
    "        df['analyze'] = df['score'].apply(analyze)              # bunuda ben yeni olusturuyorum\n",
    "        st.write(df)                                            #tüm yorumlari görmek istedigim icin df.head(10) gibi bir sinirlama yapmadim\n",
    "\n",
    "    if upl:\n",
    "        df = pd.read_excel(upl)\n",
    "        df['score'] = df['COMMENTS'].apply(score)\n",
    "        df['analyze'] = df['score'].apply(analyze)\n",
    "        st.write(df)\n",
    "\n",
    "        #bu okuma islemi her bir comment'te calisacak, ve herbirini okuyacak\n",
    "\n",
    "\n",
    "        @st.cache                                               # bu sekilde eger bu dosyayi bir daha okutmak istersen, öncekileri hafizasinda turuyor, ve yenileri aliyor\n",
    "                                                                # bu sekilde daha az maliyetli bir calisma olur\n",
    "        def convert_df(df):\n",
    "\n",
    "            return df.to_csv().encode('utf-8')                  #bu sekilde ise, datayi excelden ceviriyor ve csv olarak kaydetmemiz icin\n",
    "\n",
    "        csv = convert_df(df)                                    #burda csv olarak datamizi yakaliyoruz, asagidaki satirlarda data=csv yazacagiz\n",
    "\n",
    "\n",
    "        st.download_button(\n",
    "            label='Download data as CSV',\n",
    "            data = csv,\n",
    "            file_name='Sentmiment.csv',\n",
    "            mime='text/csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd                                                             # textleri tablo halinde göstermek ve CSV dosyalarini kullanmak icin\n",
    "from textblob import TextBlob                                                   # https://textblob.readthedocs.io/en/dev/index.html\n",
    "import streamlit as st\n",
    "import cleantext\n",
    "\n",
    "\n",
    "\n",
    "st.header('The-Rugs Sentiment Analysis')\n",
    "\n",
    "\n",
    "with st.expander('Analyze Text'):\n",
    "    text = st.text_input('Text here please')                                     # buda bir alt satir olusturuyor ve icine yapmasi gerekenleri söylüyorsun\n",
    "    if text:                                                                     # burda yani o expander icinde bir metin varsa, yani bos degilse, anlamina geliyor,biseler olunca demek\n",
    "        blob = TextBlob(text)                                                    # girilen metni bu hazir library degerlendiriyor\n",
    "        st.write('Polarity: ', round(blob.sentiment.polarity, 2))                # polarity library icinde hazir hesaplanan degeri alacagiz.pretrained edilmis bir kütüphane\n",
    "        st.write('Subjectivity: ', round(blob.sentiment.subjectivity, 2))\n",
    "        def output_sentiment():\n",
    "            if round(blob.sentiment.polarity, 2) > 0.5:\n",
    "                output_positive = 'Positive'\n",
    "                return output_positive\n",
    "            else:\n",
    "                output_negative = 'Negative'\n",
    "                return output_negative\n",
    "        st.write('Sentiment : ', output_sentiment())                             # bu fonksiyon olarak koydum, ve ona sira geldiginde hemen calismasi icin\n",
    "\n",
    "    pre = st.text_input('Clean Text : ')                                         # bu da orda bir satir acti, su an biz expander Analyze Text altindayiz\n",
    "    if pre:\n",
    "        st.write(cleantext.clean(pre, clean_all=False, extra_spaces=True, stopwords= True, lowercase=True,\n",
    "                                 numbers=True, punct=True))                      # numaralari ve punctuation silmek icin, burda stopwordleri silerken not olanlari silmiyor,denedim\n",
    "\n",
    "with st.expander('Analyze The Entire File'):\n",
    "    upl = st.file_uploader('Upload File')                                        # bunun ile dosyayi yüklemek icin pencere aciliyor, ve dosyayi secme islemi yapiliyor\n",
    "\n",
    "    def score(x):\n",
    "        blob1 = TextBlob(x)\n",
    "        return blob1.sentiment.polarity\n",
    "\n",
    "\n",
    "\n",
    "    def analyze(x):\n",
    "        if x >= 0.5:\n",
    "            return 'Positive'\n",
    "        elif x<= -0.5:\n",
    "            return 'Negative'\n",
    "        else:\n",
    "            return 'Neutral'\n",
    "\n",
    "\n",
    "    if upl:\n",
    "        df = pd.read_excel(upl)\n",
    "        #del df['Unnamed: 0']\n",
    "        df['score'] = df['COMMENTS'].apply(score)               #bu sütunu ben kendim yeni olusturuyorum, icindeki score ise yukarida tanimladigim bir function\n",
    "        df['analyze'] = df['score'].apply(analyze)              # bunuda ben yeni olusturuyorum\n",
    "        st.write(df)                                            #tüm yorumlari görmek istedigim icin df.head(10) gibi bir sinirlama yapmadim\n",
    "\n",
    "    if upl:\n",
    "        df = pd.read_excel(upl)\n",
    "        df['score'] = df['COMMENTS'].apply(score)\n",
    "        df['analyze'] = df['score'].apply(analyze)\n",
    "        st.write(df)\n",
    "\n",
    "        #bu okuma islemi her bir comment'te calisacak, ve herbirini okuyacak\n",
    "\n",
    "\n",
    "        @st.cache                                               # bu sekilde eger bu dosyayi bir daha okutmak istersen, öncekileri hafizasinda turuyor, ve yenileri aliyor\n",
    "                                                                # bu sekilde daha az maliyetli bir calisma olur\n",
    "        def convert_df(df):\n",
    "\n",
    "            return df.to_csv().encode('utf-8')                  #bu sekilde ise, datayi excelden ceviriyor ve csv olarak kaydetmemiz icin\n",
    "\n",
    "        csv = convert_df(df)                                    #burda csv olarak datamizi yakaliyoruz, asagidaki satirlarda data=csv yazacagiz\n",
    "\n",
    "\n",
    "        st.download_button(\n",
    "            label='Download data as CSV',\n",
    "            data = csv,\n",
    "            file_name='Sentmiment.csv',\n",
    "            mime='text/csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f7c19a79b9aeb8b1cc18eda6778f62d726c8b19540b84d23ad80114035b2e0b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
